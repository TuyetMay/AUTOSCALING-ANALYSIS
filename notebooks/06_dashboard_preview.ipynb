{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f92d8ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cell 1 done — paths ready + CFG ready (CFG['SCALING'] exists)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 1 — Config & Paths (clean + judge-friendly)\n",
    "# - Tách rõ: DATA/MODEL vs AUTOSCALING/SIM\n",
    "# - Window-aware + Metric-aware (buffer/capacity)\n",
    "# - Không phá các helper đã dùng ở cell sau\n",
    "# =========================\n",
    "\n",
    "import os, re, json, math\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "from pathlib import Path\n",
    "import os, shutil\n",
    "\n",
    "# Root luôn là thư mục notebooks (vì notebook nằm trong notebooks/)\n",
    "PROJECT_ROOT = Path.cwd()   # => .../AUTOSCALING-ANALYSIS/notebooks\n",
    "\n",
    "# (Optional) Nếu data đang nằm ở ../data thì copy vào notebooks/data để mọi thứ \"nằm trong notebooks\"\n",
    "src_data = (PROJECT_ROOT / \"..\" / \"data\").resolve()\n",
    "dst_data = (PROJECT_ROOT / \"data\").resolve()\n",
    "\n",
    "if not (dst_data / \"raw\").exists() and (src_data / \"raw\").exists():\n",
    "    dst_data.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copytree(src_data, dst_data, dirs_exist_ok=True)\n",
    "    print(f\"✅ Copied data from {src_data} -> {dst_data}\")\n",
    "\n",
    "PROJECT_ROOT = str(PROJECT_ROOT)  # giữ kiểu string cho code hiện tại\n",
    "OUT_02 = os.path.join(PROJECT_ROOT, \"outputs\", \"02_eda\")\n",
    "OUT_03 = os.path.join(PROJECT_ROOT, \"outputs\", \"03_features\")\n",
    "OUT_04 = os.path.join(PROJECT_ROOT, \"outputs\", \"04_models\")\n",
    "OUT_04P = os.path.join(OUT_04, \"predictions\")\n",
    "OUT_05 = os.path.join(PROJECT_ROOT, \"outputs\", \"05_scaling\")\n",
    "\n",
    "for p in [OUT_02, OUT_03, OUT_04, OUT_04P, OUT_05]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Core helpers (keep as-is for other cells)\n",
    "# -------------------------\n",
    "def tag_minutes(tag: str) -> int:\n",
    "    return {\"1m\": 1, \"5m\": 5, \"15m\": 15}[tag]\n",
    "\n",
    "def steps_per_day(tag: str) -> int:\n",
    "    return int(24 * 60 / tag_minutes(tag))\n",
    "\n",
    "def steps_per_hour(tag: str) -> int:\n",
    "    return int(60 / tag_minutes(tag))\n",
    "\n",
    "def resolve_roll_windows(tag: str, roll_windows: List[str]) -> Dict[str, int]:\n",
    "    sph = steps_per_hour(tag)\n",
    "    spd = steps_per_day(tag)\n",
    "    out = {}\n",
    "    for w in roll_windows:\n",
    "        if w == \"1h\":\n",
    "            out[w] = 1 * sph\n",
    "        elif w == \"6h\":\n",
    "            out[w] = 6 * sph\n",
    "        elif w == \"1d\":\n",
    "            out[w] = 1 * spd\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported roll window: {w}\")\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# CFG (one source of truth)\n",
    "# -------------------------\n",
    "CFG: Dict[str, Any] = {\n",
    "    # ===== Dataset =====\n",
    "    \"RAW_LOG_PATH\": os.path.join(PROJECT_ROOT, \"data\", \"access_log.txt\"),  # optional\n",
    "    \"TAGS\": [\"1m\", \"5m\", \"15m\"],\n",
    "    \"TIME_COL_RAW\": \"timestamp\",\n",
    "    \"TIME_COL_BUCKET\": \"bucket_start\",\n",
    "\n",
    "    # Storm gap (problem statement)\n",
    "    \"STORM_START\": pd.Timestamp(\"1995-08-01 14:52:01\"),\n",
    "    \"STORM_END\":   pd.Timestamp(\"1995-08-03 04:36:13\"),\n",
    "\n",
    "    # ===== Feature engineering =====\n",
    "    \"LAG_DAYS\": [1,2,3,4,5,6,7],\n",
    "    \"ROLL_WINDOWS\": [\"1h\",\"6h\",\"1d\"],\n",
    "    \"ROLL_USE_STD\": True,\n",
    "    \"USE_CYCLIC\": True,\n",
    "    \"HORIZON_STEPS\": 1,\n",
    "    \"KEEP_RAW_EXTRA\": [\n",
    "        \"unique_hosts\",\"err_4xx\",\"err_5xx\",\"error_rate\",\n",
    "        \"is_missing_bucket\",\"is_gap_storm\",\"is_gap_unknown\"\n",
    "    ],\n",
    "    \"REQUIRE_COLS\": [\"bucket_start\",\"hits\",\"bytes_sum\",\"is_gap\"],\n",
    "\n",
    "    # ===== Modeling =====\n",
    "    \"TARGETS\": [\"hits\", \"bytes_sum\"],\n",
    "    \"XGB_PARAMS\": dict(\n",
    "        booster=\"gbtree\",\n",
    "        n_estimators=5000,\n",
    "        early_stopping_rounds=50,\n",
    "        objective=\"reg:squarederror\",\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    \"CV_SPLITS\": 5,\n",
    "    \"CV_TEST_DAYS\": 2,\n",
    "    \"CV_GAP_STEPS\": 1,\n",
    "\n",
    "    # ==========================================================\n",
    "    # AUTOSCALING / SIMULATION CONFIG (Window-aware + Metric-aware)\n",
    "    # ==========================================================\n",
    "    \"SCALING\": {\n",
    "        # bounds\n",
    "        \"min_instances\": 2,\n",
    "        \"max_instances\": 50,\n",
    "\n",
    "        # unit cost\n",
    "        \"cost_per_instance_per_hour\": 0.05,\n",
    "\n",
    "        # window -> minutes\n",
    "        \"window_minutes\": {\"1m\": 1, \"5m\": 5, \"15m\": 15},\n",
    "\n",
    "        # --- Metric-aware safety buffer (tránh bytes_sum bị under-provision)\n",
    "        # hits thường ổn với buffer vừa; bytes_sum hay burst => buffer cao hơn\n",
    "        \"safety_buffer_by_metric\": {\"hits\": 0.3, \"bytes_sum\": 0.3},\n",
    "\n",
    "        # --- Per-instance capacity (tune để required_instances có dao động đẹp)\n",
    "        # NOTE: nếu muốn demo \"predictive có phản ứng\", hạ bytes_sum cap xuống\n",
    "        \"capacity_per_instance\": {\n",
    "            (\"hits\",\"1m\"): 20, (\"hits\",\"5m\"): 100, (\"hits\",\"15m\"): 350,\n",
    "            (\"bytes_sum\",\"1m\"): 350_000, (\"bytes_sum\",\"5m\"): 1_200_000, (\"bytes_sum\",\"15m\"): 3_500_000,\n",
    "        },\n",
    "\n",
    "        # --- Step change per window (15m không nên nhảy quá lớn cho đẹp)\n",
    "        \"max_step_change_by_window\": {\"1m\": 6, \"5m\": 10, \"15m\": 15},\n",
    "\n",
    "        # --- Hysteresis per window (1m noise => high/low lớn hơn)\n",
    "        # high: số cửa sổ liên tiếp vượt ngưỡng mới scale-out\n",
    "        # low : số cửa sổ liên tiếp dưới ngưỡng mới scale-in\n",
    "        \"hysteresis_by_window\": {\n",
    "            \"1m\": {\"high\": 2, \"low\": 6, \"in_margin\": 0.18},\n",
    "            \"5m\": {\"high\": 1, \"low\": 4, \"in_margin\": 0.15},\n",
    "            \"15m\":{\"high\": 1, \"low\": 2, \"in_margin\": 0.12},\n",
    "        },\n",
    "\n",
    "        \"predictive_deadband_by_window\": {\"1m\": 0.5, \"5m\": 0.5, \"15m\": 0.5},\n",
    "\n",
    "        # --- cooldown (tính theo phút, convert trong code)\n",
    "        \"cooldown_minutes\": {\"base\": 8, \"spike\": 15},\n",
    "\n",
    "        # --- provisioning per window\n",
    "        \"provisioning_by_window\": {\n",
    "            \"1m\": {\"warmup_windows\": 1, \"min_uptime_windows\": 6},\n",
    "            \"5m\": {\"warmup_windows\": 1, \"min_uptime_windows\": 4},\n",
    "            \"15m\":{\"warmup_windows\": 0, \"min_uptime_windows\": 2},\n",
    "        },\n",
    "\n",
    "        # --- Reactive (rescue) knobs\n",
    "        \"reactive\": {\n",
    "            \"enabled\": True,\n",
    "            \"overload_scale_out_immediate\": True,\n",
    "            \"rescue_extra_instances\": 3,\n",
    "            \"queue_low_fraction\": 0.05,\n",
    "            \"queue_high_multiplier\": 4.0,  # cao hơn để giảm false rescue => đẹp demo\n",
    "        },\n",
    "\n",
    "        # --- SLO / latency model (đơn giản hóa)\n",
    "        \"slo\": {\n",
    "            \"base_latency_ms\": 80.0,\n",
    "            \"alpha_latency_per_unit_queue\": 0.15,\n",
    "            \"p95_latency_target_ms\": 300.0,\n",
    "        },\n",
    "\n",
    "        # --- Anomaly detection (MAD) theo lookback giờ (convert trong code)\n",
    "        \"anomaly\": {\n",
    "            \"enabled\": True,\n",
    "            \"method\": \"mad\",\n",
    "            \"lookback_hours\": 2,\n",
    "            \"mad_k\": 6.0,\n",
    "            \"min_points\": 10,\n",
    "            \"max_flag_rate\": 0.30,\n",
    "        },\n",
    "\n",
    "        # --- DDoS mode (force step per window)\n",
    "        \"ddos_mode\": {\n",
    "            \"enabled\": True,\n",
    "            \"force_scale_out_step_by_window\": {\"1m\": 6, \"5m\": 10, \"15m\": 12},\n",
    "            \"max_instances_during_ddos\": 50,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Cell 1 done — paths ready + CFG ready (CFG['SCALING'] exists)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4393e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ generated:\n",
      "- C:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\streamlit_app.py\n",
      "- C:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\05_scaling\\_cfg_scaling.json\n",
      "\\nRun: streamlit run notebooks\\streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 06_dashboard_preview.ipynb\n",
    "# Generates streamlit_app.py (dashboard) consistent with artifacts in outputs/*\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# notebook nằm trong notebooks/ => root của dashboard cũng là notebooks/\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "PROJECT_ROOT = PROJECT_ROOT.resolve()\n",
    "\n",
    "OUT_05 = PROJECT_ROOT / \"outputs\" / \"05_scaling\"\n",
    "OUT_05.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "app_path = PROJECT_ROOT / \"streamlit_app.py\"\n",
    "\n",
    "\n",
    "# Convert tuple keys to strings for JSON export\n",
    "SC = CFG[\"SCALING\"].copy()\n",
    "cap_map={}\n",
    "for (m,w), v in SC[\"capacity_per_instance\"].items():\n",
    "    cap_map[f\"{m}__{w}\"]=v\n",
    "SC[\"capacity_per_instance\"]=cap_map\n",
    "\n",
    "cfg_scaling_path = OUT_05 / \"_cfg_scaling.json\"\n",
    "pd.Series(SC).to_json(cfg_scaling_path)\n",
    "\n",
    "\n",
    "APP = '''\n",
    "# streamlit_app.py\n",
    "# ============================================================\n",
    "# Streamlit Dashboard — Forecast + Autoscaling (consistent with notebook cells)\n",
    "# - Reads artifacts in outputs/*\n",
    "# - Sidebar controls WORK (form + session_state)\n",
    "# - Has KPI cards + p95 latency + queue/util charts\n",
    "# - Fixes KeyError: ('hits','5m') by normalizing capacity_per_instance keys\n",
    "# ============================================================\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Basic page config\n",
    "# -----------------------------\n",
    "st.set_page_config(page_title=\"Autoscaling Analysis\", layout=\"wide\")\n",
    "np.random.seed(42)\n",
    "\n",
    "from pathlib import Path\n",
    "import os, math, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Root = thư mục chứa streamlit_app.py  => .../AUTOSCALING-ANALYSIS/notebooks\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent\n",
    "\n",
    "# (Optional) Nếu data đang nằm ở ../data thì copy vào notebooks/data\n",
    "src_data = (PROJECT_ROOT / \"..\" / \"data\").resolve()\n",
    "dst_data = (PROJECT_ROOT / \"data\").resolve()\n",
    "if not (dst_data / \"raw\").exists() and (src_data / \"raw\").exists():\n",
    "    dst_data.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copytree(src_data, dst_data, dirs_exist_ok=True)\n",
    "\n",
    "OUT_04P = str(PROJECT_ROOT / \"outputs\" / \"04_models\" / \"predictions\")\n",
    "OUT_04  = str(PROJECT_ROOT / \"outputs\" / \"04_models\")\n",
    "OUT_05  = str(PROJECT_ROOT / \"outputs\" / \"05_scaling\")\n",
    "METRICS_PATH = str(PROJECT_ROOT / \"outputs\" / \"04_models\" / \"metrics_forecast.csv\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CFG (copy from CELL 1 — keep consistent)\n",
    "# NOTE: tuple keys in capacity_per_instance will be normalized for Streamlit.\n",
    "# ============================================================\n",
    "CFG = {\n",
    "    \"TAGS\": [\"1m\", \"5m\", \"15m\"],\n",
    "    \"TARGETS\": [\"hits\", \"bytes_sum\"],\n",
    "    \"SCALING\": {\n",
    "        \"min_instances\": 2,\n",
    "        \"max_instances\": 50,\n",
    "        \"cost_per_instance_per_hour\": 0.05,\n",
    "        \"window_minutes\": {\"1m\": 1, \"5m\": 5, \"15m\": 15},\n",
    "        \"safety_buffer_by_metric\": {\"hits\": 0.3, \"bytes_sum\": 0.3},\n",
    "        \"capacity_per_instance\": {\n",
    "            (\"hits\",\"1m\"): 20, (\"hits\",\"5m\"): 100, (\"hits\",\"15m\"): 350,\n",
    "            (\"bytes_sum\",\"1m\"): 350_000, (\"bytes_sum\",\"5m\"): 1_200_000, (\"bytes_sum\",\"15m\"): 3_500_000,\n",
    "        },\n",
    "        \"max_step_change_by_window\": {\"1m\": 6, \"5m\": 10, \"15m\": 15},\n",
    "        \"hysteresis_by_window\": {\n",
    "            \"1m\": {\"high\": 2, \"low\": 6, \"in_margin\": 0.18},\n",
    "            \"5m\": {\"high\": 2, \"low\": 4, \"in_margin\": 0.18},\n",
    "            \"15m\": {\"high\": 1, \"low\": 2, \"in_margin\": 0.12},\n",
    "        },\n",
    "        \"cooldown_minutes\": {\"base\": 15, \"spike\": 15},\n",
    "        \"provisioning_by_window\": {\n",
    "            \"1m\": {\"warmup_windows\": 1, \"min_uptime_windows\": 6},\n",
    "            \"5m\": {\"warmup_windows\": 1, \"min_uptime_windows\": 4},\n",
    "            \"15m\":{\"warmup_windows\": 0, \"min_uptime_windows\": 2},\n",
    "        },\n",
    "        \"slo\": {\n",
    "            \"base_latency_ms\": 80.0,\n",
    "            \"alpha_latency_per_unit_queue\": 0.15,\n",
    "            \"p95_latency_target_ms\": 300.0,\n",
    "        },\n",
    "        \"anomaly\": {\n",
    "            \"enabled\": True,\n",
    "            \"lookback_hours\": 2,\n",
    "            \"mad_k\": 6.0,\n",
    "            \"min_points\": 10,\n",
    "        },\n",
    "        \"ddos_mode\": {\n",
    "            \"enabled\": True,\n",
    "            \"force_scale_out_step_by_window\": {\"1m\": 6, \"5m\": 10, \"15m\": 12},\n",
    "            \"max_instances_during_ddos\": 50,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "SC = CFG[\"SCALING\"]\n",
    "\n",
    "# ============================================================\n",
    "# Normalizers / helpers (fix tuple-key issues)\n",
    "# ============================================================\n",
    "def normalize_capacity_keys(cap_dict):\n",
    "    \"\"\"\n",
    "    Ensure SC[\"capacity_per_instance\"] supports (metric, window) lookup.\n",
    "    Handles cases where dict keys might become strings when copied/loaded elsewhere.\n",
    "    \"\"\"\n",
    "    if not isinstance(cap_dict, dict):\n",
    "        return {}\n",
    "\n",
    "    out = {}\n",
    "    for k, v in cap_dict.items():\n",
    "        if isinstance(k, tuple) and len(k) == 2:\n",
    "            out[(str(k[0]), str(k[1]))] = float(v)\n",
    "        elif isinstance(k, str):\n",
    "            ks = k.strip()\n",
    "            # common patterns:\n",
    "            # \"('hits', '5m')\" or \"hits,5m\" or \"hits|5m\"\n",
    "            if ks.startswith(\"(\") and \",\" in ks:\n",
    "                try:\n",
    "                    # very small, safe parse (no eval)\n",
    "                    ks2 = ks.strip(\"()\")\n",
    "                    a, b = ks2.split(\",\", 1)\n",
    "                    a = a.strip().strip(\"'\").strip('\"')\n",
    "                    b = b.strip().strip(\"'\").strip('\"')\n",
    "                    out[(a, b)] = float(v)\n",
    "                    continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if \"|\" in ks:\n",
    "                a, b = ks.split(\"|\", 1)\n",
    "                out[(a.strip(), b.strip())] = float(v)\n",
    "                continue\n",
    "            if \",\" in ks:\n",
    "                a, b = ks.split(\",\", 1)\n",
    "                out[(a.strip(), b.strip())] = float(v)\n",
    "                continue\n",
    "        # ignore unknown key formats\n",
    "    return out\n",
    "\n",
    "SC[\"capacity_per_instance\"] = normalize_capacity_keys(SC.get(\"capacity_per_instance\", {}))\n",
    "\n",
    "def win_minutes(window: str) -> int:\n",
    "    return int(SC[\"window_minutes\"][window])\n",
    "\n",
    "def win_hours(window: str) -> float:\n",
    "    return win_minutes(window) / 60.0\n",
    "\n",
    "def clamp_instances(x: int) -> int:\n",
    "    return max(int(SC[\"min_instances\"]), min(int(SC[\"max_instances\"]), int(x)))\n",
    "\n",
    "def cap(metric: str, window: str) -> float:\n",
    "    key = (str(metric), str(window))\n",
    "    if key not in SC[\"capacity_per_instance\"]:\n",
    "        raise KeyError(f\"capacity_per_instance missing key={key}. Please check CFG['SCALING']['capacity_per_instance'].\")\n",
    "    return float(SC[\"capacity_per_instance\"][key])\n",
    "\n",
    "def buffer(metric: str) -> float:\n",
    "    return float(SC[\"safety_buffer_by_metric\"].get(metric, 0.2))\n",
    "\n",
    "def step_limit(window: str) -> int:\n",
    "    return int(SC[\"max_step_change_by_window\"].get(window, 10))\n",
    "\n",
    "def required_instances(demand: float, metric: str, window: str) -> int:\n",
    "    d = max(0.0, float(demand))\n",
    "    c = max(cap(metric, window), 1e-9)\n",
    "    need = (d / c) * (1.0 + buffer(metric))\n",
    "    return clamp_instances(int(math.ceil(need)))\n",
    "\n",
    "def _apply_step_towards(inst, target, max_step):\n",
    "    delta = int(target) - int(inst)\n",
    "    if delta == 0:\n",
    "        return int(inst)\n",
    "    step = int(np.sign(delta)) * min(abs(delta), int(max_step))\n",
    "    return clamp_instances(int(inst) + step)\n",
    "\n",
    "def daily_event_counts(ev_df: pd.DataFrame):\n",
    "    if ev_df is None or ev_df.empty:\n",
    "        return pd.DataFrame(columns=[\"date\",\"scale_out\",\"scale_in\",\"total\"])\n",
    "    tmp = ev_df.copy()\n",
    "    tmp[\"timestamp\"] = pd.to_datetime(tmp[\"timestamp\"])\n",
    "    tmp[\"date\"] = tmp[\"timestamp\"].dt.date.astype(str)\n",
    "    tmp[\"is_out\"] = tmp[\"action\"].astype(str).str.contains(\"scale_out\", case=False, na=False).astype(int)\n",
    "    tmp[\"is_in\"]  = tmp[\"action\"].astype(str).str.contains(\"scale_in\",  case=False, na=False).astype(int)\n",
    "    g = tmp.groupby(\"date\")[[\"is_out\",\"is_in\"]].sum().reset_index().rename(columns={\"is_out\":\"scale_out\",\"is_in\":\"scale_in\"})\n",
    "    g[\"total\"] = g[\"scale_out\"] + g[\"scale_in\"]\n",
    "    return g.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "def instance_distribution(sim_df: pd.DataFrame):\n",
    "    g = sim_df[\"instances\"].astype(int).value_counts().sort_index()\n",
    "    out = pd.DataFrame({\"instances\": g.index, \"count\": g.values})\n",
    "    out[\"pct_time\"] = out[\"count\"] / out[\"count\"].sum()\n",
    "    return out\n",
    "\n",
    "def mad_anomaly_flags(series: pd.Series, window_pts: int, k: float, min_points: int = 10):\n",
    "    x = series.astype(float).copy()\n",
    "    mp = max(1, min(int(window_pts), max(int(min_points), int(window_pts)//2)))\n",
    "    med = x.rolling(int(window_pts), min_periods=mp).median()\n",
    "    mad = (x - med).abs().rolling(int(window_pts), min_periods=mp).median()\n",
    "    score = (x - med).abs() / mad.replace(0, np.nan)\n",
    "    is_spike = (score > k).fillna(False).astype(int)\n",
    "    return score.fillna(0.0), is_spike\n",
    "\n",
    "def ddos_flag(is_spike: pd.Series, consec: int):\n",
    "    run = is_spike.rolling(consec, min_periods=consec).sum()\n",
    "    return (run >= consec).fillna(False).astype(int)\n",
    "\n",
    "def simulate_queue_latency(sim_df: pd.DataFrame, lat_cfg: dict):\n",
    "    df = sim_df.sort_values(\"timestamp\").reset_index(drop=True).copy()\n",
    "    q = 0.0\n",
    "    ql, util, p95, slo = [], [], [], []\n",
    "    for _, r in df.iterrows():\n",
    "        load = float(r[\"y_true\"])\n",
    "        cap_total = float(r[\"capacity_total\"])\n",
    "        _served = min(load + q, cap_total)\n",
    "        q = max(0.0, (load + q) - cap_total)\n",
    "        q = max(0.0, q * (1.0 - float(lat_cfg[\"queue_decay\"])))\n",
    "        u = 0.0 if cap_total <= 1e-9 else min(2.0, load / cap_total)\n",
    "        p = float(lat_cfg[\"base_ms\"]) + float(lat_cfg[\"alpha_ms_per_queue_unit\"]) * q\n",
    "        v = bool(p > float(lat_cfg[\"p95_target_ms\"]))\n",
    "        ql.append(float(q)); util.append(float(u)); p95.append(float(p)); slo.append(v)\n",
    "    df[\"queue_len\"] = ql\n",
    "    df[\"utilization\"] = util\n",
    "    df[\"p95_latency_ms\"] = p95\n",
    "    df[\"slo_violation\"] = pd.Series(slo).astype(bool)\n",
    "    return df\n",
    "\n",
    "def summarize(sim_df: pd.DataFrame, ev_df: pd.DataFrame) -> dict:\n",
    "    metric = str(sim_df[\"metric\"].iloc[0])\n",
    "    window = str(sim_df[\"window\"].iloc[0])\n",
    "    policy = str(sim_df[\"policy_mode\"].iloc[0])\n",
    "    n = int(len(sim_df))\n",
    "    total_cost = float(sim_df[\"cost_step\"].sum())\n",
    "    total_server_hours = float(sim_df[\"server_hours_step\"].sum())\n",
    "    avg_instances = float(sim_df[\"instances\"].mean())\n",
    "    peak_instances = int(sim_df[\"instances\"].max())\n",
    "    sla_violation_rate = float(sim_df[\"sla_violation\"].mean())\n",
    "    slo_violation_rate = float(sim_df[\"slo_violation\"].mean()) if \"slo_violation\" in sim_df.columns else np.nan\n",
    "    total_under = float(sim_df[\"under_provision\"].sum())\n",
    "    max_under = float(sim_df[\"under_provision\"].max())\n",
    "    num_scale_events = int(len(ev_df)) if ev_df is not None else 0\n",
    "    sim_hours = (n * win_minutes(window)) / 60.0\n",
    "    events_per_hour = float(num_scale_events / max(sim_hours, 1e-9))\n",
    "    return {\n",
    "        \"metric\": metric, \"window\": window, \"policy_mode\": policy,\n",
    "        \"estimated_total_cost\": total_cost,\n",
    "        \"total_server_hours\": total_server_hours,\n",
    "        \"avg_instances\": avg_instances,\n",
    "        \"peak_instances\": peak_instances,\n",
    "        \"sla_violation_rate\": sla_violation_rate,\n",
    "        \"slo_violation_rate\": float(slo_violation_rate),\n",
    "        \"total_under_provision\": total_under,\n",
    "        \"max_under_provision\": max_under,\n",
    "        \"num_scale_events\": num_scale_events,\n",
    "        \"events_per_hour\": events_per_hour,\n",
    "        \"num_points\": n,\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# Data loaders (cache MUST include params)\n",
    "# ============================================================\n",
    "@st.cache_data(show_spinner=False)\n",
    "def load_pred_case(metric: str, window: str, model_tag: str) -> pd.DataFrame:\n",
    "    fp = os.path.join(OUT_04P, f\"pred_{metric}_{window}_{model_tag}.csv\")\n",
    "    if not os.path.exists(fp):\n",
    "        raise FileNotFoundError(fp)\n",
    "\n",
    "    dfp = pd.read_csv(fp)\n",
    "    # robust timestamp normalize\n",
    "    ts = pd.to_datetime(dfp[\"bucket_start\"], utc=True, errors=\"coerce\").dt.tz_convert(None)\n",
    "    dfp = dfp.assign(timestamp=ts).dropna(subset=[\"timestamp\"]).copy()\n",
    "\n",
    "    df_case = pd.DataFrame({\n",
    "        \"timestamp\": dfp[\"timestamp\"],\n",
    "        \"y_true\": pd.to_numeric(dfp[metric], errors=\"coerce\").fillna(0.0),\n",
    "        \"y_pred\": pd.to_numeric(dfp[\"pred\"], errors=\"coerce\").fillna(0.0),\n",
    "    }).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    return df_case\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def load_metrics_long() -> pd.DataFrame:\n",
    "    if not os.path.exists(METRICS_PATH):\n",
    "        return pd.DataFrame(columns=[\"model\",\"target\",\"window\",\"split\",\"metric\",\"value\"])\n",
    "    return pd.read_csv(METRICS_PATH)\n",
    "\n",
    "def get_forecast_kpis(metrics_df: pd.DataFrame, model: str, target: str, window: str):\n",
    "    x = metrics_df.copy()\n",
    "    if x.empty:\n",
    "        return {\"RMSE\": np.nan, \"MAE\": np.nan, \"MAPE\": np.nan}\n",
    "    x[\"split\"] = x[\"split\"].astype(str).str.lower()\n",
    "    x[\"model\"] = x[\"model\"].astype(str)\n",
    "    x[\"target\"] = x[\"target\"].astype(str)\n",
    "    x[\"window\"] = x[\"window\"].astype(str)\n",
    "    filt = x[(x[\"split\"]==\"test\") & (x[\"model\"]==model) & (x[\"target\"]==target) & (x[\"window\"]==window)]\n",
    "    if filt.empty:\n",
    "        return {\"RMSE\": np.nan, \"MAE\": np.nan, \"MAPE\": np.nan}\n",
    "    out = {}\n",
    "    for k in [\"RMSE\",\"MAE\",\"MAPE\"]:\n",
    "        r = filt[filt[\"metric\"]==k][\"value\"]\n",
    "        out[k] = float(r.iloc[0]) if len(r) else np.nan\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# Simulation (consistent with CELL10)\n",
    "# ============================================================\n",
    "def simulate_static(df_case: pd.DataFrame, metric: str, window: str, static_n: int):\n",
    "    df = df_case.sort_values(\"timestamp\").reset_index(drop=True).copy()\n",
    "    wh = win_hours(window)\n",
    "    unit_cost = float(SC[\"cost_per_instance_per_hour\"])\n",
    "    inst = clamp_instances(static_n)\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        ts = r[\"timestamp\"]\n",
    "        y_true = float(r[\"y_true\"])\n",
    "        y_pred = float(r[\"y_pred\"])\n",
    "        capacity_total = inst * cap(metric, window)\n",
    "        headroom = capacity_total - y_true\n",
    "        under = max(0.0, -headroom)\n",
    "        over = max(0.0, headroom)\n",
    "        cost_step = inst * wh * unit_cost\n",
    "        rows.append({\n",
    "            \"timestamp\": ts, \"metric\": metric, \"window\": window, \"policy_mode\": \"static\",\n",
    "            \"y_true\": y_true, \"y_pred\": y_pred,\n",
    "            \"required_instances\": int(required_instances(y_pred, metric, window)),\n",
    "            \"instances\": int(inst), \"effective_instances\": int(inst),\n",
    "            \"capacity_total\": float(capacity_total),\n",
    "            \"headroom\": float(headroom),\n",
    "            \"under_provision\": float(under), \"over_provision\": float(over),\n",
    "            \"sla_violation\": bool(under > 0.0),\n",
    "            \"cost_step\": float(cost_step),\n",
    "            \"server_hours_step\": float(inst * wh),\n",
    "            \"cost_rate_per_hour\": float(inst * unit_cost),\n",
    "        })\n",
    "    sim_df = pd.DataFrame(rows)\n",
    "    ev_df = pd.DataFrame(columns=[\"timestamp\",\"metric\",\"window\",\"policy_mode\",\"action\",\"from_instances\",\"to_instances\",\"delta\",\"reason\"])\n",
    "    return sim_df, ev_df\n",
    "\n",
    "def simulate_predictive(df_case: pd.DataFrame, metric: str, window: str,\n",
    "                        hysteresis_high: int, hysteresis_low: int, in_margin: float,\n",
    "                        cooldown_minutes: float, max_step: int,\n",
    "                        warmup_windows: int, min_uptime_windows: int,\n",
    "                        enable_anom: bool, lookback_hours: float, mad_k: float, min_points: int,\n",
    "                        enable_ddos: bool, ddos_consec: int, ddos_force_step: int, ddos_max: int,\n",
    "                        lat_cfg: dict):\n",
    "    df = df_case.sort_values(\"timestamp\").reset_index(drop=True).copy()\n",
    "    wh = win_hours(window)\n",
    "    unit_cost = float(SC[\"cost_per_instance_per_hour\"])\n",
    "\n",
    "    cooldown_w = int(math.ceil(float(cooldown_minutes) / win_minutes(window)))\n",
    "\n",
    "    inst = int(SC[\"min_instances\"])\n",
    "    above_count = below_count = 0\n",
    "    cooldown_left = warmup_left = uptime_guard = 0\n",
    "    rows, events = [], []\n",
    "\n",
    "    # precompute anomaly/ddos flags from actual load (demo)\n",
    "    if enable_anom:\n",
    "        lookback_pts = max(5, int((lookback_hours * 60) / win_minutes(window)))\n",
    "        an_score, is_spike = mad_anomaly_flags(df[\"y_true\"], lookback_pts, mad_k, min_points)\n",
    "        is_ddos = ddos_flag(is_spike, ddos_consec) if enable_ddos else pd.Series(np.zeros(len(df), dtype=int))\n",
    "    else:\n",
    "        an_score = pd.Series(np.zeros(len(df)))\n",
    "        is_spike = pd.Series(np.zeros(len(df), dtype=int))\n",
    "        is_ddos  = pd.Series(np.zeros(len(df), dtype=int))\n",
    "\n",
    "    for i, r in df.iterrows():\n",
    "        ts = r[\"timestamp\"]\n",
    "        y_true = float(r[\"y_true\"])\n",
    "        y_pred = float(r[\"y_pred\"])\n",
    "\n",
    "        req = required_instances(y_pred, metric, window)\n",
    "\n",
    "        # hysteresis counters\n",
    "        above_count = (above_count + 1) if (req > inst) else 0\n",
    "        below_count = (below_count + 1) if (inst > req * (1.0 + float(in_margin))) else 0\n",
    "\n",
    "        # tick timers\n",
    "        cooldown_left = max(0, cooldown_left - 1)\n",
    "        warmup_left = max(0, warmup_left - 1)\n",
    "        uptime_guard = max(0, uptime_guard - 1)\n",
    "\n",
    "        action, reason = \"hold\", \"hold\"\n",
    "        new_inst = inst\n",
    "\n",
    "        ddos_on = bool(enable_ddos and int(is_ddos.iloc[i]) == 1)\n",
    "\n",
    "        # ddos mode: force scale-out\n",
    "        if ddos_on:\n",
    "            new_inst = min(int(ddos_max), inst + max(1, int(ddos_force_step)))\n",
    "            action, reason = \"scale_out\", \"ddos_mode(force_step)\"\n",
    "\n",
    "        # normal predictive scaling\n",
    "        elif cooldown_left == 0:\n",
    "            if above_count >= int(hysteresis_high):\n",
    "                new_inst = _apply_step_towards(inst, req, int(max_step))\n",
    "                action, reason = \"scale_out\", f\"req>inst for {int(hysteresis_high)} window(s)\"\n",
    "            elif below_count >= int(hysteresis_low) and uptime_guard == 0:\n",
    "                new_inst = _apply_step_towards(inst, req, int(max_step))\n",
    "                action, reason = \"scale_in\", f\"inst>req*(1+margin) for {int(hysteresis_low)} window(s)\"\n",
    "\n",
    "        if new_inst != inst:\n",
    "            events.append({\n",
    "                \"timestamp\": ts, \"metric\": metric, \"window\": window, \"policy_mode\": \"predictive\",\n",
    "                \"action\": action, \"from_instances\": int(inst), \"to_instances\": int(new_inst),\n",
    "                \"delta\": int(new_inst - inst), \"reason\": reason\n",
    "            })\n",
    "            inst = int(new_inst)\n",
    "            cooldown_left = cooldown_w\n",
    "            warmup_left = max(warmup_left, int(warmup_windows))\n",
    "            uptime_guard = max(uptime_guard, int(min_uptime_windows))\n",
    "\n",
    "        effective_inst = max(0, inst - warmup_left)\n",
    "        capacity_total = effective_inst * cap(metric, window)\n",
    "        headroom = capacity_total - y_true\n",
    "        under = max(0.0, -headroom)\n",
    "        over = max(0.0, headroom)\n",
    "        cost_step = inst * wh * unit_cost\n",
    "\n",
    "        rows.append({\n",
    "            \"timestamp\": ts, \"metric\": metric, \"window\": window, \"policy_mode\": \"predictive\",\n",
    "            \"y_true\": y_true, \"y_pred\": y_pred,\n",
    "            \"required_instances\": int(req),\n",
    "            \"instances\": int(inst),\n",
    "            \"effective_instances\": int(effective_inst),\n",
    "            \"warmup_left_windows\": int(warmup_left),\n",
    "            \"blocked_by_cooldown\": bool(cooldown_left > 0),\n",
    "            \"capacity_total\": float(capacity_total),\n",
    "            \"headroom\": float(headroom),\n",
    "            \"under_provision\": float(under), \"over_provision\": float(over),\n",
    "            \"sla_violation\": bool(under > 0.0),\n",
    "            \"cost_step\": float(cost_step),\n",
    "            \"server_hours_step\": float(inst * wh),\n",
    "            \"cost_rate_per_hour\": float(inst * unit_cost),\n",
    "            \"anomaly_score\": float(an_score.iloc[i]),\n",
    "            \"is_spike\": int(is_spike.iloc[i]),\n",
    "            \"is_ddos\": int(is_ddos.iloc[i]),\n",
    "        })\n",
    "\n",
    "    sim_pred = pd.DataFrame(rows)\n",
    "    ev_pred = pd.DataFrame(events)\n",
    "    sim_pred = simulate_queue_latency(sim_pred, lat_cfg)\n",
    "    return sim_pred, ev_pred\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Sidebar (WORKS): use form -> Apply/Run\n",
    "# ============================================================\n",
    "st.title(\"Autoscaling Analysis — Forecast + Policy Simulation\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"Controls\")\n",
    "\n",
    "    st.caption(\"Dataset split (read-only)\")\n",
    "    st.write(\"Train = Jul + 1–22 Aug\")\n",
    "    st.write(\"Test  = 23–31 Aug\")\n",
    "\n",
    "    with st.form(\"cfg_form\"):\n",
    "        st.subheader(\"A) Data / Range\")\n",
    "        # default test range as per spec\n",
    "        test_start = st.date_input(\"Test start (inclusive)\", value=pd.Timestamp(\"1995-08-23\").date(), key=\"test_start\")\n",
    "        test_end   = st.date_input(\"Test end (exclusive)\", value=pd.Timestamp(\"1995-09-01\").date(), key=\"test_end\")\n",
    "        include_gaps = st.checkbox(\"Include gaps\", value=False, key=\"include_gaps\")\n",
    "\n",
    "        st.subheader(\"B) Forecast\")\n",
    "        metric = st.selectbox(\"Target (metric)\", CFG[\"TARGETS\"], index=0, key=\"metric\")\n",
    "        window = st.selectbox(\"Window\", CFG[\"TAGS\"], index=1, key=\"window\")\n",
    "        model_tag = st.selectbox(\"Model\", [\"xgb\", \"seasonal_naive\"], index=0, key=\"model\")\n",
    "\n",
    "        st.subheader(\"C) Autoscaling policy\")\n",
    "        policy_mode = st.selectbox(\"Policy mode\", [\"static_vs_predictive\"], index=0, key=\"policy_mode\")\n",
    "\n",
    "        # allow user adjust core knobs\n",
    "        buf = st.slider(\"Safety buffer\", 0.0, 1.0, float(SC[\"safety_buffer_by_metric\"].get(metric, 0.3)), 0.01, key=\"buf\")\n",
    "        min_ins = st.number_input(\"Min instances\", 1, 500, int(SC[\"min_instances\"]), key=\"min_ins\")\n",
    "        max_ins = st.number_input(\"Max instances\", 1, 2000, int(SC[\"max_instances\"]), key=\"max_ins\")\n",
    "\n",
    "        st.caption(\"Hysteresis / cooldown / step\")\n",
    "        hhigh = st.number_input(\"high (scale-out consecutive windows)\", 1, 20, int(SC[\"hysteresis_by_window\"][window][\"high\"]), key=\"hhigh\")\n",
    "        hlow  = st.number_input(\"low (scale-in consecutive windows)\", 1, 50, int(SC[\"hysteresis_by_window\"][window][\"low\"]), key=\"hlow\")\n",
    "        in_margin = st.slider(\"in_margin\", 0.0, 1.0, float(SC[\"hysteresis_by_window\"][window][\"in_margin\"]), 0.01, key=\"in_margin\")\n",
    "        cooldown_m = st.number_input(\"Cooldown minutes\", 0.0, 120.0, float(SC[\"cooldown_minutes\"][\"base\"]), step=1.0, key=\"cooldown_m\")\n",
    "        max_step = st.number_input(\"Step limit (max change per decision)\", 1, 200, int(SC[\"max_step_change_by_window\"][window]), key=\"max_step\")\n",
    "\n",
    "        st.caption(\"Provisioning\")\n",
    "        warmup_w = st.number_input(\"Warmup windows\", 0, 50, int(SC[\"provisioning_by_window\"][window][\"warmup_windows\"]), key=\"warmup_w\")\n",
    "        min_uptime_w = st.number_input(\"Min uptime windows\", 0, 200, int(SC[\"provisioning_by_window\"][window][\"min_uptime_windows\"]), key=\"min_uptime_w\")\n",
    "\n",
    "        st.subheader(\"D) Anomaly / DDoS\")\n",
    "        enable_anom = st.checkbox(\"Enable anomaly (MAD)\", value=bool(SC[\"anomaly\"][\"enabled\"]), key=\"enable_anom\")\n",
    "        lookback_h = st.number_input(\"lookback_hours\", 0.5, 24.0, float(SC[\"anomaly\"][\"lookback_hours\"]), step=0.5, key=\"lookback_h\")\n",
    "        mad_k = st.number_input(\"mad_k\", 1.0, 20.0, float(SC[\"anomaly\"][\"mad_k\"]), step=0.5, key=\"mad_k\")\n",
    "        min_pts = st.number_input(\"min_points\", 1, 200, int(SC[\"anomaly\"][\"min_points\"]), key=\"min_pts\")\n",
    "\n",
    "        enable_ddos = st.checkbox(\"Enable DDoS mode\", value=bool(SC[\"ddos_mode\"][\"enabled\"]), key=\"enable_ddos\")\n",
    "        ddos_consec = st.number_input(\"consecutive_windows\", 1, 20, 3, key=\"ddos_consec\")\n",
    "        ddos_force_step = st.number_input(\"force_step\", 1, 200, int(SC[\"ddos_mode\"][\"force_scale_out_step_by_window\"][window]), key=\"ddos_force_step\")\n",
    "        ddos_max = st.number_input(\"max_instances_during_ddos\", 1, 5000, int(SC[\"ddos_mode\"][\"max_instances_during_ddos\"]), key=\"ddos_max\")\n",
    "\n",
    "        run = st.form_submit_button(\"Apply / Run simulation\")\n",
    "\n",
    "# ============================================================\n",
    "# Apply sidebar -> update SC\n",
    "# ============================================================\n",
    "SC[\"safety_buffer_by_metric\"][metric] = float(buf)\n",
    "SC[\"min_instances\"] = int(min_ins)\n",
    "SC[\"max_instances\"] = int(max_ins)\n",
    "\n",
    "SC[\"hysteresis_by_window\"][window][\"high\"] = int(hhigh)\n",
    "SC[\"hysteresis_by_window\"][window][\"low\"]  = int(hlow)\n",
    "SC[\"hysteresis_by_window\"][window][\"in_margin\"] = float(in_margin)\n",
    "SC[\"cooldown_minutes\"][\"base\"] = float(cooldown_m)\n",
    "SC[\"max_step_change_by_window\"][window] = int(max_step)\n",
    "SC[\"provisioning_by_window\"][window][\"warmup_windows\"] = int(warmup_w)\n",
    "SC[\"provisioning_by_window\"][window][\"min_uptime_windows\"] = int(min_uptime_w)\n",
    "\n",
    "SC[\"anomaly\"][\"enabled\"] = bool(enable_anom)\n",
    "SC[\"anomaly\"][\"lookback_hours\"] = float(lookback_h)\n",
    "SC[\"anomaly\"][\"mad_k\"] = float(mad_k)\n",
    "SC[\"anomaly\"][\"min_points\"] = int(min_pts)\n",
    "\n",
    "SC[\"ddos_mode\"][\"enabled\"] = bool(enable_ddos)\n",
    "SC[\"ddos_mode\"][\"force_scale_out_step_by_window\"][window] = int(ddos_force_step)\n",
    "SC[\"ddos_mode\"][\"max_instances_during_ddos\"] = int(ddos_max)\n",
    "\n",
    "LAT_CFG = {\n",
    "    \"base_ms\": float(SC[\"slo\"][\"base_latency_ms\"]),\n",
    "    \"alpha_ms_per_queue_unit\": float(SC[\"slo\"][\"alpha_latency_per_unit_queue\"]),\n",
    "    \"p95_target_ms\": float(SC[\"slo\"][\"p95_latency_target_ms\"]),\n",
    "    \"queue_decay\": 0.02,\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# Run simulation (only when button pressed OR first load)\n",
    "# ============================================================\n",
    "if \"sim_pred\" not in st.session_state:\n",
    "    st.session_state[\"sim_pred\"] = None\n",
    "    st.session_state[\"sim_static\"] = None\n",
    "    st.session_state[\"ev_pred\"] = None\n",
    "    st.session_state[\"summary_df\"] = None\n",
    "\n",
    "def run_all():\n",
    "    df_case = load_pred_case(metric, window, model_tag)\n",
    "\n",
    "    # filter by selected date range (test)\n",
    "    TEST_START = pd.Timestamp(test_start)\n",
    "    TEST_END   = pd.Timestamp(test_end)  # exclusive (date_input gives date)\n",
    "    df_case = df_case[(df_case[\"timestamp\"] >= TEST_START) & (df_case[\"timestamp\"] < TEST_END)].copy()\n",
    "    df_case = df_case.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    # include/exclude gaps (if your pred files already removed gaps, this mostly no-op)\n",
    "    if not include_gaps:\n",
    "        # keep all rows (pred files generally already filtered)\n",
    "        pass\n",
    "\n",
    "    # \"train\" for static baseline (here: use df_case itself if no earlier history in this view)\n",
    "    # If you want exact spec baseline (Jul + 1–22 Aug), you should load TRAIN pred file separately.\n",
    "    # For demo: use robust fallback if train slice empty.\n",
    "    TRAIN_END = pd.Timestamp(\"1995-08-23 00:00:00\")\n",
    "    df_train = df_case[df_case[\"timestamp\"] < TRAIN_END].copy()\n",
    "    if df_train.empty:\n",
    "        cut = int(len(df_case) * 0.7)\n",
    "        df_train = df_case.iloc[:cut].copy()\n",
    "\n",
    "    static_req_train = df_train[\"y_true\"].apply(lambda x: required_instances(x, metric, window))\n",
    "    static_n = clamp_instances(int(np.nanpercentile(static_req_train.values, 95)))\n",
    "\n",
    "    sim_static, ev_static = simulate_static(df_case, metric, window, static_n)\n",
    "    sim_static = simulate_queue_latency(sim_static, LAT_CFG)\n",
    "\n",
    "    sim_pred, ev_pred = simulate_predictive(\n",
    "        df_case, metric, window,\n",
    "        hysteresis_high=int(hhigh),\n",
    "        hysteresis_low=int(hlow),\n",
    "        in_margin=float(in_margin),\n",
    "        cooldown_minutes=float(cooldown_m),\n",
    "        max_step=int(max_step),\n",
    "        warmup_windows=int(warmup_w),\n",
    "        min_uptime_windows=int(min_uptime_w),\n",
    "        enable_anom=bool(enable_anom),\n",
    "        lookback_hours=float(lookback_h),\n",
    "        mad_k=float(mad_k),\n",
    "        min_points=int(min_pts),\n",
    "        enable_ddos=bool(enable_ddos),\n",
    "        ddos_consec=int(ddos_consec),\n",
    "        ddos_force_step=int(ddos_force_step),\n",
    "        ddos_max=int(ddos_max),\n",
    "        lat_cfg=LAT_CFG,\n",
    "    )\n",
    "\n",
    "    summary_static = summarize(sim_static, ev_static)\n",
    "    summary_pred   = summarize(sim_pred, ev_pred)\n",
    "    summary_df = pd.DataFrame([summary_static, summary_pred])\n",
    "\n",
    "    # Save artifacts (optional)\n",
    "    os.makedirs(OUT_05, exist_ok=True)\n",
    "    sim_all = pd.concat([sim_static, sim_pred], ignore_index=True)\n",
    "    events_all = pd.concat([ev_static, ev_pred], ignore_index=True)\n",
    "    sim_all.to_csv(os.path.join(OUT_05, \"sim_timeseries_all.csv\"), index=False)\n",
    "    events_all.to_csv(os.path.join(OUT_05, \"scaling_events_all.csv\"), index=False)\n",
    "    summary_df.to_csv(os.path.join(OUT_05, \"summary_cost_perf.csv\"), index=False)\n",
    "\n",
    "    st.session_state[\"sim_pred\"] = sim_pred\n",
    "    st.session_state[\"sim_static\"] = sim_static\n",
    "    st.session_state[\"ev_pred\"] = ev_pred\n",
    "    st.session_state[\"summary_df\"] = summary_df\n",
    "    st.session_state[\"static_n\"] = int(static_n)\n",
    "\n",
    "if run or (st.session_state[\"sim_pred\"] is None):\n",
    "    try:\n",
    "        run_all()\n",
    "    except Exception as e:\n",
    "        st.error(f\"Run failed: {e}\")\n",
    "        st.stop()\n",
    "\n",
    "sim_pred = st.session_state[\"sim_pred\"]\n",
    "sim_static = st.session_state[\"sim_static\"]\n",
    "ev_pred = st.session_state[\"ev_pred\"]\n",
    "summary_df = st.session_state[\"summary_df\"]\n",
    "static_n = st.session_state.get(\"static_n\", None)\n",
    "\n",
    "# ============================================================\n",
    "# Metrics + KPIs\n",
    "# ============================================================\n",
    "metrics_long = load_metrics_long()\n",
    "fk = get_forecast_kpis(metrics_long, model_tag, metric, window)\n",
    "\n",
    "cost_static = float(summary_df[summary_df[\"policy_mode\"]==\"static\"][\"estimated_total_cost\"].iloc[0])\n",
    "cost_pred   = float(summary_df[summary_df[\"policy_mode\"]==\"predictive\"][\"estimated_total_cost\"].iloc[0])\n",
    "events_per_hour = float(summary_df[summary_df[\"policy_mode\"]==\"predictive\"][\"events_per_hour\"].iloc[0])\n",
    "sla_rate = float(summary_df[summary_df[\"policy_mode\"]==\"predictive\"][\"sla_violation_rate\"].iloc[0])\n",
    "slo_rate = float(summary_df[summary_df[\"policy_mode\"]==\"predictive\"][\"slo_violation_rate\"].iloc[0])\n",
    "\n",
    "# ============================================================\n",
    "# Tabs\n",
    "# ============================================================\n",
    "tab_overview, tab_forecast, tab_scale, tab_cost, tab_anom = st.tabs(\n",
    "    [\"Overview\", \"Forecast\", \"Autoscaling\", \"Cost vs Reliability\", \"Anomaly/DDoS\"]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Overview\n",
    "# -----------------------------\n",
    "with tab_overview:\n",
    "    st.subheader(\"KPI Summary\")\n",
    "\n",
    "    c1, c2, c3, c4, c5 = st.columns(5)\n",
    "    c1.metric(\"RMSE (test)\", \"NA\" if np.isnan(fk[\"RMSE\"]) else f\"{fk['RMSE']:.3f}\")\n",
    "    c2.metric(\"MAE (test)\",  \"NA\" if np.isnan(fk[\"MAE\"])  else f\"{fk['MAE']:.3f}\")\n",
    "    c3.metric(\"MAPE (test)\", \"NA\" if np.isnan(fk[\"MAPE\"]) else f\"{fk['MAPE']:.2f}%\")\n",
    "    c4.metric(\"Total cost (Static)\", f\"${cost_static:.2f}\")\n",
    "    c5.metric(\"Total cost (Predictive)\", f\"${cost_pred:.2f}\", delta=f\"{(cost_pred-cost_static):.2f}\")\n",
    "\n",
    "    c6, c7, c8, c9 = st.columns(4)\n",
    "    c6.metric(\"SLA violation rate\", f\"{sla_rate*100:.2f}%\")\n",
    "    c7.metric(\"SLO violation rate\", f\"{slo_rate*100:.2f}%\")\n",
    "    c8.metric(\"# events (predictive)\", f\"{len(ev_pred) if ev_pred is not None else 0}\")\n",
    "    c9.metric(\"Events/hour\", f\"{events_per_hour:.2f}\")\n",
    "\n",
    "    st.caption(f\"Static baseline instances (p95 train-style): {static_n}\")\n",
    "\n",
    "    # Overview charts\n",
    "    ts = sim_pred[\"timestamp\"]\n",
    "\n",
    "    colA, colB = st.columns(2)\n",
    "    with colA:\n",
    "        fig = plt.figure(figsize=(12,3))\n",
    "        plt.plot(ts, sim_pred[\"y_true\"], label=\"Actual\")\n",
    "        plt.plot(ts, sim_pred[\"y_pred\"], label=\"Forecast\")\n",
    "        plt.title(\"Actual vs Forecast\"); plt.xlabel(\"time\"); plt.grid(True); plt.legend()\n",
    "        st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "    with colB:\n",
    "        fig = plt.figure(figsize=(12,3))\n",
    "        plt.plot(ts, sim_pred[\"required_instances\"], label=\"Desired\")\n",
    "        plt.plot(ts, sim_pred[\"instances\"], label=\"InService\")\n",
    "        plt.title(\"Desired vs InService\"); plt.xlabel(\"time\"); plt.grid(True); plt.legend()\n",
    "        st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(14,3))\n",
    "    plt.plot(sim_static[\"timestamp\"], sim_static[\"cost_step\"].cumsum(), label=\"STATIC cumulative cost\")\n",
    "    plt.plot(sim_pred[\"timestamp\"], sim_pred[\"cost_step\"].cumsum(), label=\"PREDICTIVE cumulative cost\")\n",
    "    plt.title(\"Cumulative Cost\"); plt.xlabel(\"time\"); plt.grid(True); plt.legend()\n",
    "    st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Forecast\n",
    "# -----------------------------\n",
    "with tab_forecast:\n",
    "    st.subheader(\"Forecast diagnostics\")\n",
    "\n",
    "    ts = sim_pred[\"timestamp\"]\n",
    "    resid = sim_pred[\"y_true\"] - sim_pred[\"y_pred\"]\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        fig = plt.figure(figsize=(12,3))\n",
    "        plt.plot(ts, sim_pred[\"y_true\"], label=\"Actual\")\n",
    "        plt.plot(ts, sim_pred[\"y_pred\"], label=\"Forecast\")\n",
    "        plt.title(\"Actual vs Forecast\"); plt.xlabel(\"time\"); plt.grid(True); plt.legend()\n",
    "        st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "        fig = plt.figure(figsize=(12,3))\n",
    "        plt.plot(ts, resid)\n",
    "        plt.title(\"Residual (y_true - y_pred)\"); plt.xlabel(\"time\"); plt.grid(True)\n",
    "        st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "    with col2:\n",
    "        fig = plt.figure(figsize=(12,3))\n",
    "        plt.hist(resid.values, bins=60)\n",
    "        plt.title(\"Residual distribution\"); plt.xlabel(\"residual\"); plt.ylabel(\"count\"); plt.grid(True)\n",
    "        st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "        fig = plt.figure(figsize=(12,3))\n",
    "        plt.scatter(sim_pred[\"y_true\"], sim_pred[\"y_pred\"], s=8)\n",
    "        plt.title(\"y_true vs y_pred\"); plt.xlabel(\"y_true\"); plt.ylabel(\"y_pred\"); plt.grid(True)\n",
    "        st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "    st.subheader(\"Benchmark table (from metrics_forecast.csv)\")\n",
    "    if metrics_long.empty:\n",
    "        st.info(\"metrics_forecast.csv not found yet.\")\n",
    "    else:\n",
    "        test_m = metrics_long[metrics_long[\"split\"].astype(str).str.lower().eq(\"test\")].copy()\n",
    "        bench = test_m.pivot_table(\n",
    "            index=[\"target\",\"window\",\"metric\"],\n",
    "            columns=[\"model\"],\n",
    "            values=\"value\",\n",
    "            aggfunc=\"first\"\n",
    "        ).reset_index()\n",
    "        st.dataframe(bench.sort_values([\"target\",\"window\",\"metric\"]), use_container_width=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Autoscaling\n",
    "# -----------------------------\n",
    "with tab_scale:\n",
    "    st.subheader(\"Policy simulation\")\n",
    "\n",
    "    ts = sim_pred[\"timestamp\"]\n",
    "\n",
    "    fig = plt.figure(figsize=(14,3))\n",
    "    plt.plot(ts, sim_pred[\"required_instances\"], label=\"Desired\")\n",
    "    plt.plot(ts, sim_pred[\"instances\"], label=\"InService\")\n",
    "    plt.title(\"Desired vs InService (NO event lines)\"); plt.xlabel(\"time\"); plt.ylabel(\"# instances\")\n",
    "    plt.grid(True); plt.legend()\n",
    "    st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(14,3))\n",
    "    plt.plot(ts, sim_pred[\"headroom\"], label=\"Headroom = capacity - load\")\n",
    "    plt.axhline(0, linewidth=1)\n",
    "    plt.title(\"Capacity Headroom (negative => SLA risk)\"); plt.xlabel(\"time\")\n",
    "    plt.grid(True); plt.legend()\n",
    "    st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        dist = instance_distribution(sim_pred)\n",
    "        fig = plt.figure(figsize=(12,3))\n",
    "        plt.bar(dist[\"instances\"].astype(str), dist[\"pct_time\"])\n",
    "        plt.title(\"Instance distribution (% time)\"); plt.xlabel(\"# instances\"); plt.ylabel(\"% time\")\n",
    "        plt.grid(True, axis=\"y\")\n",
    "        st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "    with col2:\n",
    "        counts = daily_event_counts(ev_pred)\n",
    "        if counts.empty:\n",
    "            st.info(\"No scaling events in this slice.\")\n",
    "        else:\n",
    "            counts[\"date\"] = pd.to_datetime(counts[\"date\"])\n",
    "            fig = plt.figure(figsize=(12,3))\n",
    "            plt.bar(counts[\"date\"], counts[\"scale_out\"], label=\"scale-out/day\")\n",
    "            plt.bar(counts[\"date\"], counts[\"scale_in\"], bottom=counts[\"scale_out\"], label=\"scale-in/day\")\n",
    "            plt.title(\"Scaling frequency (events/day)\"); plt.xlabel(\"date\"); plt.ylabel(\"# events/day\")\n",
    "            plt.grid(True, axis=\"y\"); plt.legend()\n",
    "            st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "    st.subheader(\"Scaling events\")\n",
    "    if ev_pred is None or ev_pred.empty:\n",
    "        st.info(\"No events.\")\n",
    "    else:\n",
    "        st.dataframe(\n",
    "            ev_pred.sort_values(\"timestamp\").reset_index(drop=True),\n",
    "            use_container_width=True\n",
    "        )\n",
    "\n",
    "# -----------------------------\n",
    "# Cost vs Reliability\n",
    "# -----------------------------\n",
    "with tab_cost:\n",
    "    st.subheader(\"Cost vs Reliability\")\n",
    "\n",
    "    fig = plt.figure(figsize=(14,3))\n",
    "    plt.plot(sim_static[\"timestamp\"], sim_static[\"cost_step\"].cumsum(), label=\"STATIC cumulative cost\")\n",
    "    plt.plot(sim_pred[\"timestamp\"], sim_pred[\"cost_step\"].cumsum(), label=\"PREDICTIVE cumulative cost\")\n",
    "    plt.title(\"Cumulative cost\"); plt.xlabel(\"time\"); plt.ylabel(\"$\")\n",
    "    plt.grid(True); plt.legend()\n",
    "    st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "    p95_target = float(LAT_CFG[\"p95_target_ms\"])\n",
    "    fig = plt.figure(figsize=(14,3))\n",
    "    plt.plot(sim_pred[\"timestamp\"], sim_pred[\"p95_latency_ms\"], label=\"p95 latency (ms)\")\n",
    "    plt.axhline(p95_target, linewidth=1, label=\"SLO target\")\n",
    "    plt.title(\"p95 latency vs SLO target\"); plt.xlabel(\"time\"); plt.ylabel(\"ms\")\n",
    "    plt.grid(True); plt.legend()\n",
    "    st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        fig = plt.figure(figsize=(12,3))\n",
    "        plt.plot(sim_pred[\"timestamp\"], sim_pred[\"queue_len\"])\n",
    "        plt.title(\"Queue length\"); plt.xlabel(\"time\"); plt.grid(True)\n",
    "        st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "    with col2:\n",
    "        fig = plt.figure(figsize=(12,3))\n",
    "        plt.plot(sim_pred[\"timestamp\"], sim_pred[\"utilization\"])\n",
    "        plt.title(\"Utilization\"); plt.xlabel(\"time\"); plt.grid(True)\n",
    "        st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Anomaly / DDoS\n",
    "# -----------------------------\n",
    "with tab_anom:\n",
    "    st.subheader(\"Spike / DDoS flags (bonus)\")\n",
    "\n",
    "    ts = sim_pred[\"timestamp\"]\n",
    "    fig = plt.figure(figsize=(14,3))\n",
    "    plt.plot(ts, sim_pred[\"y_true\"], label=\"Actual load\")\n",
    "    sp = sim_pred[sim_pred[\"is_spike\"] == 1]\n",
    "    dd = sim_pred[sim_pred[\"is_ddos\"] == 1]\n",
    "    if not sp.empty:\n",
    "        plt.scatter(sp[\"timestamp\"], sp[\"y_true\"], s=18, label=\"Spike (MAD)\")\n",
    "    if not dd.empty:\n",
    "        plt.scatter(dd[\"timestamp\"], dd[\"y_true\"], s=26, label=\"DDoS (consecutive spikes)\")\n",
    "    plt.title(\"Actual load + spike/ddos markers\"); plt.xlabel(\"time\"); plt.ylabel(f\"{metric}/{window}\")\n",
    "    plt.grid(True); plt.legend()\n",
    "    st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(14,3))\n",
    "    plt.plot(ts, sim_pred[\"anomaly_score\"])\n",
    "    plt.title(\"Anomaly score (MAD z-like)\"); plt.xlabel(\"time\"); plt.grid(True)\n",
    "    st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "    st.subheader(\"Top anomalies\")\n",
    "    topa = sim_pred.sort_values(\"anomaly_score\", ascending=False).head(50)[\n",
    "        [\"timestamp\",\"y_true\",\"y_pred\",\"anomaly_score\",\"is_spike\",\"is_ddos\",\"instances\",\"required_instances\",\"headroom\"]\n",
    "    ]\n",
    "    st.dataframe(topa, use_container_width=True)\n",
    "\n",
    "# ============================================================\n",
    "# Export buttons\n",
    "# ============================================================\n",
    "st.divider()\n",
    "colx, coly, colz = st.columns(3)\n",
    "with colx:\n",
    "    st.download_button(\n",
    "        \"Download summary_cost_perf.csv\",\n",
    "        data=summary_df.to_csv(index=False).encode(\"utf-8-sig\"),\n",
    "        file_name=\"summary_cost_perf.csv\",\n",
    "        mime=\"text/csv\",\n",
    "    )\n",
    "with coly:\n",
    "    st.download_button(\n",
    "        \"Download sim_timeseries_predictive.csv\",\n",
    "        data=sim_pred.to_csv(index=False).encode(\"utf-8-sig\"),\n",
    "        file_name=\"sim_timeseries_predictive.csv\",\n",
    "        mime=\"text/csv\",\n",
    "    )\n",
    "with colz:\n",
    "    if ev_pred is None:\n",
    "        ev_bytes = pd.DataFrame().to_csv(index=False).encode(\"utf-8-sig\")\n",
    "    else:\n",
    "        ev_bytes = ev_pred.to_csv(index=False).encode(\"utf-8-sig\")\n",
    "    st.download_button(\n",
    "        \"Download scaling_events_predictive.csv\",\n",
    "        data=ev_bytes,\n",
    "        file_name=\"scaling_events_predictive.csv\",\n",
    "        mime=\"text/csv\",\n",
    "    )\n",
    "'''\n",
    "\n",
    "with open(app_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(APP)\n",
    "print(\"✅ generated:\")\n",
    "print(\"-\", str(app_path))\n",
    "print(\"-\", str(cfg_scaling_path))\n",
    "print(r\"\\nRun: streamlit run notebooks\\streamlit_app.py\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
