{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cell 1 done — paths ready + CFG ready (CFG['SCALING'] exists)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 1 — Config & Paths (judge-friendly + reproducible)\n",
    "# - No hard-coded absolute paths\n",
    "# - Works whether you run notebook from repo root or from notebooks/\n",
    "# - DOES NOT copy data anywhere (judge-friendly)\n",
    "# - Uses pathlib for cross-platform paths\n",
    "# =========================\n",
    "\n",
    "import os, re, json, math\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility seed\n",
    "# -------------------------\n",
    "SEED = int(os.environ.get(\"SEED\", \"42\"))\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Paths (robust for Jupyter)\n",
    "# -------------------------\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_repo_root(start: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Find project root by walking up until a folder containing 'notebooks' is found.\n",
    "    Fallback: use current working dir.\n",
    "    \"\"\"\n",
    "    start = start.resolve()\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"notebooks\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "# In Jupyter, __file__ may not exist. Use cwd.\n",
    "CWD = Path.cwd().resolve()\n",
    "REPO_ROOT = _find_repo_root(CWD)\n",
    "\n",
    "# If notebook is in notebooks/, keep NOTEBOOKS_DIR = REPO_ROOT/notebooks\n",
    "NOTEBOOKS_DIR = (REPO_ROOT / \"notebooks\").resolve()\n",
    "\n",
    "# Data/outputs location:\n",
    "# - Prefer notebooks/data if exists\n",
    "# - Else fallback to repo_root/data\n",
    "DATA_DIR = (NOTEBOOKS_DIR / \"data\") if (NOTEBOOKS_DIR / \"data\").exists() else (REPO_ROOT / \"data\")\n",
    "\n",
    "# Outputs always under notebooks/outputs to match your current structure\n",
    "OUT_DIR = NOTEBOOKS_DIR / \"outputs\"\n",
    "OUT_02 = OUT_DIR / \"02_eda\"\n",
    "OUT_03 = OUT_DIR / \"03_features\"\n",
    "OUT_04 = OUT_DIR / \"04_models\"\n",
    "OUT_04P = OUT_04 / \"predictions\"\n",
    "OUT_05 = OUT_DIR / \"05_scaling\"\n",
    "\n",
    "for p in [OUT_02, OUT_03, OUT_04, OUT_04P, OUT_05]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Provide string paths if later cells use os.path.join / strings\n",
    "PROJECT_ROOT = str(NOTEBOOKS_DIR)  # keep compatible with later cells that expect PROJECT_ROOT as string\n",
    "\n",
    "print(\"✅ Paths resolved:\")\n",
    "print(\" - REPO_ROOT     :\", REPO_ROOT)\n",
    "print(\" - NOTEBOOKS_DIR :\", NOTEBOOKS_DIR)\n",
    "print(\" - DATA_DIR      :\", DATA_DIR)\n",
    "print(\" - OUT_DIR       :\", OUT_DIR)\n",
    "\n",
    "# -------------------------\n",
    "# Core helpers (keep as-is for other cells)\n",
    "# -------------------------\n",
    "def tag_minutes(tag: str) -> int:\n",
    "    return {\"1m\": 1, \"5m\": 5, \"15m\": 15}[tag]\n",
    "\n",
    "def steps_per_day(tag: str) -> int:\n",
    "    return int(24 * 60 / tag_minutes(tag))\n",
    "\n",
    "def steps_per_hour(tag: str) -> int:\n",
    "    return int(60 / tag_minutes(tag))\n",
    "\n",
    "def resolve_roll_windows(tag: str, roll_windows: List[str]) -> Dict[str, int]:\n",
    "    sph = steps_per_hour(tag)\n",
    "    spd = steps_per_day(tag)\n",
    "    out = {}\n",
    "    for w in roll_windows:\n",
    "        if w == \"1h\":\n",
    "            out[w] = 1 * sph\n",
    "        elif w == \"6h\":\n",
    "            out[w] = 6 * sph\n",
    "        elif w == \"1d\":\n",
    "            out[w] = 1 * spd\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported roll window: {w}\")\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# CFG (one source of truth)\n",
    "# -------------------------\n",
    "CFG: Dict[str, Any] = {\n",
    "    # ===== Dataset =====\n",
    "    # Prefer access_log.txt in DATA_DIR; judge just needs to put data in ./data or ./notebooks/data\n",
    "    \"RAW_LOG_PATH\": str(DATA_DIR / \"access_log.txt\"),\n",
    "    \"TAGS\": [\"1m\", \"5m\", \"15m\"],\n",
    "    \"TIME_COL_RAW\": \"timestamp\",\n",
    "    \"TIME_COL_BUCKET\": \"bucket_start\",\n",
    "\n",
    "    # Storm gap (problem statement)\n",
    "    \"STORM_START\": pd.Timestamp(\"1995-08-01 14:52:01\"),\n",
    "    \"STORM_END\":   pd.Timestamp(\"1995-08-03 04:36:13\"),\n",
    "\n",
    "    # ===== Feature engineering =====\n",
    "    \"LAG_DAYS\": [1,2,3,4,5,6,7],\n",
    "    \"ROLL_WINDOWS\": [\"1h\",\"6h\",\"1d\"],\n",
    "    \"ROLL_USE_STD\": True,\n",
    "    \"USE_CYCLIC\": True,\n",
    "    \"HORIZON_STEPS\": 1,\n",
    "    \"KEEP_RAW_EXTRA\": [\n",
    "        \"unique_hosts\",\"err_4xx\",\"err_5xx\",\"error_rate\",\n",
    "        \"is_missing_bucket\",\"is_gap_storm\",\"is_gap_unknown\"\n",
    "    ],\n",
    "    \"REQUIRE_COLS\": [\"bucket_start\",\"hits\",\"bytes_sum\",\"is_gap\"],\n",
    "\n",
    "    # ===== Modeling =====\n",
    "    \"TARGETS\": [\"hits\", \"bytes_sum\"],\n",
    "    \"XGB_PARAMS\": dict(\n",
    "        booster=\"gbtree\",\n",
    "        n_estimators=5000,\n",
    "        early_stopping_rounds=50,\n",
    "        objective=\"reg:squarederror\",\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=SEED,\n",
    "    ),\n",
    "    \"CV_SPLITS\": 5,\n",
    "    \"CV_TEST_DAYS\": 2,\n",
    "    \"CV_GAP_STEPS\": 1,\n",
    "\n",
    "    # ==========================================================\n",
    "    # AUTOSCALING / SIMULATION CONFIG (Window-aware + Metric-aware)\n",
    "    # ==========================================================\n",
    "    \"SCALING\": {\n",
    "        \"min_instances\": 2,\n",
    "        \"max_instances\": 50,\n",
    "        \"cost_per_instance_per_hour\": 0.05,\n",
    "        \"window_minutes\": {\"1m\": 1, \"5m\": 5, \"15m\": 15},\n",
    "        \"safety_buffer_by_metric\": {\"hits\": 0.3, \"bytes_sum\": 0.3},\n",
    "        \"capacity_per_instance\": {\n",
    "            (\"hits\",\"1m\"): 20, (\"hits\",\"5m\"): 100, (\"hits\",\"15m\"): 350,\n",
    "            (\"bytes_sum\",\"1m\"): 350_000, (\"bytes_sum\",\"5m\"): 1_200_000, (\"bytes_sum\",\"15m\"): 3_500_000,\n",
    "        },\n",
    "        \"max_step_change_by_window\": {\"1m\": 6, \"5m\": 10, \"15m\": 15},\n",
    "        \"hysteresis_by_window\": {\n",
    "            \"1m\": {\"high\": 2, \"low\": 6, \"in_margin\": 0.18},\n",
    "            \"5m\": {\"high\": 1, \"low\": 4, \"in_margin\": 0.15},\n",
    "            \"15m\":{\"high\": 1, \"low\": 2, \"in_margin\": 0.12},\n",
    "        },\n",
    "        \"predictive_deadband_by_window\": {\"1m\": 0.5, \"5m\": 0.5, \"15m\": 0.5},\n",
    "        \"cooldown_minutes\": {\"base\": 8, \"spike\": 15},\n",
    "        \"provisioning_by_window\": {\n",
    "            \"1m\": {\"warmup_windows\": 1, \"min_uptime_windows\": 6},\n",
    "            \"5m\": {\"warmup_windows\": 1, \"min_uptime_windows\": 4},\n",
    "            \"15m\":{\"warmup_windows\": 0, \"min_uptime_windows\": 2},\n",
    "        },\n",
    "        \"reactive\": {\n",
    "            \"enabled\": True,\n",
    "            \"overload_scale_out_immediate\": True,\n",
    "            \"rescue_extra_instances\": 3,\n",
    "            \"queue_low_fraction\": 0.05,\n",
    "            \"queue_high_multiplier\": 4.0,\n",
    "        },\n",
    "        \"slo\": {\n",
    "            \"base_latency_ms\": 80.0,\n",
    "            \"alpha_latency_per_unit_queue\": 0.15,\n",
    "            \"p95_latency_target_ms\": 300.0,\n",
    "        },\n",
    "        \"anomaly\": {\n",
    "            \"enabled\": True,\n",
    "            \"method\": \"mad\",\n",
    "            \"lookback_hours\": 2,\n",
    "            \"mad_k\": 6.0,\n",
    "            \"min_points\": 10,\n",
    "            \"max_flag_rate\": 0.30,\n",
    "        },\n",
    "        \"ddos_mode\": {\n",
    "            \"enabled\": True,\n",
    "            \"force_scale_out_step_by_window\": {\"1m\": 6, \"5m\": 10, \"15m\": 12},\n",
    "            \"max_instances_during_ddos\": 50,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Cell 1 done — CFG ready (CFG['SCALING'] exists)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee570921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data dir: C:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\data\n",
      "TRAIN_LOG_PATH: c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\data\\raw\\train.txt\n",
      "TEST_LOG_PATH : c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\data\\raw\\test.txt\n",
      "raw_train: (2934932, 8) | raw_test: (526648, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>host</th>\n",
       "      <th>method</th>\n",
       "      <th>url</th>\n",
       "      <th>version</th>\n",
       "      <th>status</th>\n",
       "      <th>bytes</th>\n",
       "      <th>bytes_missing_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1995-07-01 00:00:01-04:00</td>\n",
       "      <td>199.72.81.55</td>\n",
       "      <td>GET</td>\n",
       "      <td>/history/apollo/</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>6245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1995-07-01 00:00:06-04:00</td>\n",
       "      <td>unicomp6.unicomp.net</td>\n",
       "      <td>GET</td>\n",
       "      <td>/shuttle/countdown/</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>3985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1995-07-01 00:00:09-04:00</td>\n",
       "      <td>199.120.110.21</td>\n",
       "      <td>GET</td>\n",
       "      <td>/shuttle/missions/sts-73/mission-sts-73.html</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>4085</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime                  host method  \\\n",
       "0 1995-07-01 00:00:01-04:00          199.72.81.55    GET   \n",
       "1 1995-07-01 00:00:06-04:00  unicomp6.unicomp.net    GET   \n",
       "2 1995-07-01 00:00:09-04:00        199.120.110.21    GET   \n",
       "\n",
       "                                            url   version  status  bytes  \\\n",
       "0                              /history/apollo/  HTTP/1.0     200   6245   \n",
       "1                           /shuttle/countdown/  HTTP/1.0     200   3985   \n",
       "2  /shuttle/missions/sts-73/mission-sts-73.html  HTTP/1.0     200   4085   \n",
       "\n",
       "   bytes_missing_flag  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>host</th>\n",
       "      <th>method</th>\n",
       "      <th>url</th>\n",
       "      <th>version</th>\n",
       "      <th>status</th>\n",
       "      <th>bytes</th>\n",
       "      <th>bytes_missing_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1995-08-23 00:00:00-04:00</td>\n",
       "      <td>ix-mia1-02.ix.netcom.com</td>\n",
       "      <td>GET</td>\n",
       "      <td>/ksc.html</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>7087</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1995-08-23 00:00:05-04:00</td>\n",
       "      <td>internet-gw.watson.ibm.com</td>\n",
       "      <td>GET</td>\n",
       "      <td>/history/apollo/pad-abort-test-2/pad-abort-tes...</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1995-08-23 00:00:06-04:00</td>\n",
       "      <td>ix-mia1-02.ix.netcom.com</td>\n",
       "      <td>GET</td>\n",
       "      <td>/images/ksclogo-medium.gif</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>5866</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime                        host method  \\\n",
       "0 1995-08-23 00:00:00-04:00    ix-mia1-02.ix.netcom.com    GET   \n",
       "1 1995-08-23 00:00:05-04:00  internet-gw.watson.ibm.com    GET   \n",
       "2 1995-08-23 00:00:06-04:00    ix-mia1-02.ix.netcom.com    GET   \n",
       "\n",
       "                                                 url   version  status  bytes  \\\n",
       "0                                          /ksc.html  HTTP/1.0     200   7087   \n",
       "1  /history/apollo/pad-abort-test-2/pad-abort-tes...  HTTP/1.0     200   1292   \n",
       "2                         /images/ksclogo-medium.gif  HTTP/1.0     200   5866   \n",
       "\n",
       "   bytes_missing_flag  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>host</th>\n",
       "      <th>method</th>\n",
       "      <th>url</th>\n",
       "      <th>version</th>\n",
       "      <th>status</th>\n",
       "      <th>bytes</th>\n",
       "      <th>bytes_missing_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2934929</th>\n",
       "      <td>1995-08-22 23:59:57-04:00</td>\n",
       "      <td>sfsp129.slip.net</td>\n",
       "      <td>GET</td>\n",
       "      <td>/images/MOSAIC-logosmall.gif</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2934930</th>\n",
       "      <td>1995-08-22 23:59:58-04:00</td>\n",
       "      <td>sfsp129.slip.net</td>\n",
       "      <td>GET</td>\n",
       "      <td>/images/USA-logosmall.gif</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2934931</th>\n",
       "      <td>1995-08-22 23:59:59-04:00</td>\n",
       "      <td>sfsp129.slip.net</td>\n",
       "      <td>GET</td>\n",
       "      <td>/images/WORLD-logosmall.gif</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>669</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         datetime              host method  \\\n",
       "2934929 1995-08-22 23:59:57-04:00  sfsp129.slip.net    GET   \n",
       "2934930 1995-08-22 23:59:58-04:00  sfsp129.slip.net    GET   \n",
       "2934931 1995-08-22 23:59:59-04:00  sfsp129.slip.net    GET   \n",
       "\n",
       "                                  url   version  status  bytes  \\\n",
       "2934929  /images/MOSAIC-logosmall.gif  HTTP/1.0     200    363   \n",
       "2934930     /images/USA-logosmall.gif  HTTP/1.0     200    234   \n",
       "2934931   /images/WORLD-logosmall.gif  HTTP/1.0     200    669   \n",
       "\n",
       "         bytes_missing_flag  \n",
       "2934929                   0  \n",
       "2934930                   0  \n",
       "2934931                   0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>host</th>\n",
       "      <th>method</th>\n",
       "      <th>url</th>\n",
       "      <th>version</th>\n",
       "      <th>status</th>\n",
       "      <th>bytes</th>\n",
       "      <th>bytes_missing_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>526645</th>\n",
       "      <td>1995-08-31 23:59:52-04:00</td>\n",
       "      <td>cys-cap-9.wyoming.com</td>\n",
       "      <td>GET</td>\n",
       "      <td>/shuttle/missions/sts-71/movies/sts-71-launch-...</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>57344</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526646</th>\n",
       "      <td>1995-08-31 23:59:52-04:00</td>\n",
       "      <td>www-c8.proxy.aol.com</td>\n",
       "      <td>GET</td>\n",
       "      <td>/icons/unknown.xbm</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>515</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526647</th>\n",
       "      <td>1995-08-31 23:59:53-04:00</td>\n",
       "      <td>cindy.yamato.ibm.co.jp</td>\n",
       "      <td>GET</td>\n",
       "      <td>/images/kscmap-small.gif</td>\n",
       "      <td>HTTP/1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>39017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        datetime                    host method  \\\n",
       "526645 1995-08-31 23:59:52-04:00   cys-cap-9.wyoming.com    GET   \n",
       "526646 1995-08-31 23:59:52-04:00    www-c8.proxy.aol.com    GET   \n",
       "526647 1995-08-31 23:59:53-04:00  cindy.yamato.ibm.co.jp    GET   \n",
       "\n",
       "                                                      url   version  status  \\\n",
       "526645  /shuttle/missions/sts-71/movies/sts-71-launch-...  HTTP/1.0     200   \n",
       "526646                                 /icons/unknown.xbm  HTTP/1.0     200   \n",
       "526647                           /images/kscmap-small.gif  HTTP/1.0     200   \n",
       "\n",
       "        bytes  bytes_missing_flag  \n",
       "526645  57344                   0  \n",
       "526646    515                   0  \n",
       "526647  39017                   0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 2 — Streaming parse raw logs -> raw_train, raw_test\n",
    "# CELL 2 — Streaming parse raw logs -> raw_train, raw_test\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Robust base dir:\n",
    "# - If CELL 1 defined DATA_DIR -> use it\n",
    "# - Else infer from current working directory (handles running from repo root or notebooks/)\n",
    "if \"DATA_DIR\" in globals():\n",
    "    _DATA_DIR = Path(DATA_DIR)\n",
    "else:\n",
    "    cwd = Path.cwd()\n",
    "    # if we're inside /notebooks -> data is ./data\n",
    "    if cwd.name.lower() == \"notebooks\":\n",
    "        _DATA_DIR = cwd / \"data\"\n",
    "    else:\n",
    "        # repo root case -> prefer ./data; if not exists then ./notebooks/data\n",
    "        _DATA_DIR = (cwd / \"data\") if (cwd / \"data\").exists() else (cwd / \"notebooks\" / \"data\")\n",
    "\n",
    "TRAIN_LOG_PATH = os.environ.get(\"TRAIN_LOG_PATH\", str(_DATA_DIR / \"raw\" / \"train.txt\"))\n",
    "TEST_LOG_PATH  = os.environ.get(\"TEST_LOG_PATH\",  str(_DATA_DIR / \"raw\" / \"test.txt\"))\n",
    "\n",
    "CHUNK_SIZE_LINES = int(os.environ.get(\"CHUNK_SIZE_LINES\", \"300000\"))\n",
    "\n",
    "print(\"Using data dir:\", _DATA_DIR.resolve())\n",
    "print(\"TRAIN_LOG_PATH:\", TRAIN_LOG_PATH)\n",
    "print(\"TEST_LOG_PATH :\", TEST_LOG_PATH)\n",
    "\n",
    "\n",
    "DT_FORMAT = \"%d/%b/%Y:%H:%M:%S %z\"\n",
    "\n",
    "LOG_RE = re.compile(\n",
    "    r'^(?P<host>\\S+)\\s+\\S+\\s+\\S+\\s+\\[(?P<ts>[^\\]]+)\\]\\s+'\n",
    "    r'\"(?P<request>[^\"]*)\"\\s+(?P<status>\\d{3})\\s+(?P<bytes>\\S+)\\s*$'\n",
    ")\n",
    "REQ_RE = re.compile(r'^(?P<method>[A-Z]+)\\s+(?P<url>\\S+)\\s+(?P<version>HTTP/\\d\\.\\d)$')\n",
    "\n",
    "def _parse_line(line: str):\n",
    "    m = LOG_RE.match(line)\n",
    "    if not m:\n",
    "        return None\n",
    "\n",
    "    host = m.group(\"host\")\n",
    "    ts_raw = m.group(\"ts\")\n",
    "    req_raw = m.group(\"request\")\n",
    "    status_raw = m.group(\"status\")\n",
    "    bytes_raw = m.group(\"bytes\")\n",
    "\n",
    "    try:\n",
    "        dt = datetime.strptime(ts_raw, DT_FORMAT)\n",
    "    except Exception:\n",
    "        dt = pd.NaT\n",
    "\n",
    "    method = url = version = \"UNKNOWN\"\n",
    "    rm = REQ_RE.match(req_raw.strip())\n",
    "    if rm:\n",
    "        method, url, version = rm.group(\"method\"), rm.group(\"url\"), rm.group(\"version\")\n",
    "\n",
    "    try:\n",
    "        status = int(status_raw)\n",
    "    except Exception:\n",
    "        status = pd.NA\n",
    "\n",
    "    if bytes_raw in (\"-\", \"\"):\n",
    "        bval, miss = pd.NA, 1\n",
    "    else:\n",
    "        try:\n",
    "            bval, miss = int(bytes_raw), 0\n",
    "        except Exception:\n",
    "            bval, miss = pd.NA, 1\n",
    "\n",
    "    return (dt, host, method, url, version, status, bval, miss)\n",
    "\n",
    "def _normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"status\"] = pd.to_numeric(df[\"status\"], errors=\"coerce\").astype(\"Int16\")\n",
    "    df[\"bytes\"] = pd.to_numeric(df[\"bytes\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"bytes_missing_flag\"] = pd.to_numeric(df[\"bytes_missing_flag\"], errors=\"coerce\").astype(\"Int8\")\n",
    "    return df\n",
    "\n",
    "def parse_file_streaming(path: str, chunk_lines: int = CHUNK_SIZE_LINES) -> pd.DataFrame:\n",
    "    parts = []\n",
    "    buf = []\n",
    "\n",
    "    with open(path, \"r\", errors=\"replace\") as f:\n",
    "        for line in f:\n",
    "            ev = _parse_line(line.rstrip(\"\\n\"))\n",
    "            if ev is None:\n",
    "                continue\n",
    "            buf.append(ev)\n",
    "\n",
    "            if len(buf) >= chunk_lines:\n",
    "                df = pd.DataFrame(buf, columns=[\n",
    "                    \"datetime\",\"host\",\"method\",\"url\",\"version\",\"status\",\"bytes\",\"bytes_missing_flag\"\n",
    "                ])\n",
    "                parts.append(_normalize_df(df))\n",
    "                buf = []\n",
    "\n",
    "    if buf:\n",
    "        df = pd.DataFrame(buf, columns=[\n",
    "            \"datetime\",\"host\",\"method\",\"url\",\"version\",\"status\",\"bytes\",\"bytes_missing_flag\"\n",
    "        ])\n",
    "        parts.append(_normalize_df(df))\n",
    "\n",
    "    return pd.concat(parts, ignore_index=True) if parts else pd.DataFrame(columns=[\n",
    "        \"datetime\",\"host\",\"method\",\"url\",\"version\",\"status\",\"bytes\",\"bytes_missing_flag\"\n",
    "    ])\n",
    "\n",
    "raw_train = parse_file_streaming(TRAIN_LOG_PATH)\n",
    "raw_test  = parse_file_streaming(TEST_LOG_PATH)\n",
    "\n",
    "print(\"raw_train:\", raw_train.shape, \"| raw_test:\", raw_test.shape)\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(raw_train.head(3))\n",
    "    display(raw_test.head(3))\n",
    "    display(raw_train.tail(3))\n",
    "    display(raw_test.tail(3))\n",
    "except Exception:\n",
    "    print(raw_train.head(3).to_string(index=False))\n",
    "    print(raw_test.head(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e65bbee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR      :"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\n",
      "DATA_DIR      : C:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\data\n",
      "OUT_DIR       : C:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\02_eda\n",
      "SAVE_TRAIN_DIR: C:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\data\\train\n",
      "SAVE_TEST_DIR : C:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\data\\test\n",
      "train/1m | rows=76,320 | range=1995-07-01 00:00:00-04:00 -> 1995-08-22 23:59:00-04:00 | missing=7,852 | gap=7,211 (storm=2,264, unknown=7,210)\n",
      "  saved: c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\02_eda\\ts3_train_1m.parquet\n",
      "  saved: c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\data\\train\\ts3_1m.parquet\n",
      "train/5m | rows=15,264 | range=1995-07-01 00:00:00-04:00 -> 1995-08-22 23:55:00-04:00 | missing=1,490 | gap=1,442 (storm=453, unknown=1,441)\n",
      "  saved: c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\02_eda\\ts3_train_5m.parquet\n",
      "  saved: c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\data\\train\\ts3_5m.parquet\n",
      "train/15m | rows=5,088 | range=1995-07-01 00:00:00-04:00 -> 1995-08-22 23:45:00-04:00 | missing=483 | gap=480 (storm=151, unknown=479)\n",
      "  saved: c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\02_eda\\ts3_train_15m.parquet\n",
      "  saved: c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\data\\train\\ts3_15m.parquet\n",
      "test/1m | rows=12,960 | range=1995-08-23 00:00:00-04:00 -> 1995-08-31 23:59:00-04:00 | missing=32 | gap=0 (storm=0, unknown=0)\n",
      "  saved: c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\02_eda\\ts3_test_1m.parquet\n",
      "  saved: c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\data\\test\\ts3_1m.parquet\n",
      "test/5m | rows=2,592 | range=1995-08-23 00:00:00-04:00 -> 1995-08-31 23:55:00-04:00 | missing=2 | gap=0 (storm=0, unknown=0)\n",
      "  saved: c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\02_eda\\ts3_test_5m.parquet\n",
      "  saved: c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\data\\test\\ts3_5m.parquet\n",
      "test/15m | rows=864 | range=1995-08-23 00:00:00-04:00 -> 1995-08-31 23:45:00-04:00 | missing=0 | gap=0 (storm=0, unknown=0)\n",
      "  saved: c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\02_eda\\ts3_test_15m.parquet\n",
      "  saved: c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\data\\test\\ts3_15m.parquet\n"
     ]
    }
   ],
   "source": [
    "# CELL 03 — TS3 ONLY + SAVE (judge-friendly paths)\n",
    "import os, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "assert \"raw_train\" in globals() and \"raw_test\" in globals(), \"Run CELL 02 first.\"\n",
    "\n",
    "# ---------\n",
    "# Robust base dir:\n",
    "# - If CELL 1 defined PROJECT_ROOT/DATA_DIR -> use them\n",
    "# - Else infer from cwd (repo root or notebooks/)\n",
    "# ---------\n",
    "cwd = Path.cwd()\n",
    "if \"PROJECT_ROOT\" in globals():\n",
    "    # PROJECT_ROOT in your CELL 1 is a string path to notebooks/\n",
    "    nb_root = Path(PROJECT_ROOT)\n",
    "    # If that path isn't real (rare), fallback to cwd\n",
    "    if not nb_root.exists():\n",
    "        nb_root = cwd if cwd.name.lower() == \"notebooks\" else (cwd / \"notebooks\")\n",
    "else:\n",
    "    nb_root = cwd if cwd.name.lower() == \"notebooks\" else (cwd / \"notebooks\")\n",
    "\n",
    "BASE_DIR = nb_root  # keep everything under notebooks/ as you designed\n",
    "\n",
    "# data dir\n",
    "if \"DATA_DIR\" in globals():\n",
    "    DATA_DIR_PATH = Path(DATA_DIR)\n",
    "else:\n",
    "    DATA_DIR_PATH = BASE_DIR / \"data\"\n",
    "\n",
    "# outputs dir\n",
    "OUT_BASE = BASE_DIR / \"outputs\"\n",
    "\n",
    "# Allow env override (still relative-safe)\n",
    "OUT_DIR = Path(os.environ.get(\"OUT_DIR_03\", str(OUT_BASE / \"02_eda\")))\n",
    "SAVE_TRAIN_DIR = Path(os.environ.get(\"SAVE_TRAIN_DIR\", str(DATA_DIR_PATH / \"train\")))\n",
    "SAVE_TEST_DIR  = Path(os.environ.get(\"SAVE_TEST_DIR\",  str(DATA_DIR_PATH / \"test\")))\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAVE_TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAVE_TEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"BASE_DIR      :\", BASE_DIR.resolve())\n",
    "print(\"DATA_DIR      :\", DATA_DIR_PATH.resolve())\n",
    "print(\"OUT_DIR       :\", OUT_DIR.resolve())\n",
    "print(\"SAVE_TRAIN_DIR:\", SAVE_TRAIN_DIR.resolve())\n",
    "print(\"SAVE_TEST_DIR :\", SAVE_TEST_DIR.resolve())\n",
    "\n",
    "FREQS = {\"1m\": \"1min\", \"5m\": \"5min\", \"15m\": \"15min\"}\n",
    "GAP_STORM_START = pd.Timestamp(\"1995-08-01 14:52:01-0400\")\n",
    "GAP_STORM_END   = pd.Timestamp(\"1995-08-03 04:36:13-0400\")\n",
    "UNKNOWN_GAP_MIN_HOURS = int(os.environ.get(\"UNKNOWN_GAP_MIN_HOURS\", \"12\"))\n",
    "FILL_COLS = [\"hits\",\"bytes_sum\",\"avg_bytes_per_req\",\"err_4xx\",\"err_5xx\",\"error_rate\",\"unique_hosts\"]\n",
    "\n",
    "def agg(raw, freq):\n",
    "    d = raw[[\"datetime\",\"host\",\"status\",\"bytes\"]].copy()\n",
    "    d[\"datetime\"] = pd.to_datetime(d[\"datetime\"], utc=False)\n",
    "    b = d[\"datetime\"].dt.floor(freq)\n",
    "    st = pd.to_numeric(d[\"status\"], errors=\"coerce\")\n",
    "    g = d.assign(bucket_start=b, bytes_num=pd.to_numeric(d[\"bytes\"], errors=\"coerce\")).groupby(\"bucket_start\", sort=True)\n",
    "    idx = g.size().index\n",
    "    ts2 = pd.DataFrame({\n",
    "        \"bucket_start\": idx,\n",
    "        \"hits\": g.size().astype(\"int64\").values,\n",
    "        \"bytes_sum\": g[\"bytes_num\"].sum(min_count=1).astype(\"float64\").reindex(idx).values,\n",
    "        \"unique_hosts\": g[\"host\"].nunique().astype(\"int64\").reindex(idx).values,\n",
    "        \"err_4xx\": st.between(400,499).groupby(b).sum().astype(\"int64\").reindex(idx, fill_value=0).values,\n",
    "        \"err_5xx\": st.between(500,599).groupby(b).sum().astype(\"int64\").reindex(idx, fill_value=0).values,\n",
    "    }).sort_values(\"bucket_start\").reset_index(drop=True)\n",
    "    ts2[\"avg_bytes_per_req\"] = np.where(ts2[\"hits\"] > 0, ts2[\"bytes_sum\"]/ts2[\"hits\"], 0.0)\n",
    "    ts2[\"error_rate\"] = np.where(ts2[\"hits\"] > 0, (ts2[\"err_4xx\"]+ts2[\"err_5xx\"])/ts2[\"hits\"], 0.0)\n",
    "    return ts2\n",
    "\n",
    "def to_ts3(ts2, freq):\n",
    "    s, e = ts2[\"bucket_start\"].min(), ts2[\"bucket_start\"].max()\n",
    "    out = pd.DataFrame({\"bucket_start\": pd.date_range(s, e, freq=freq, tz=s.tz)}).merge(ts2, on=\"bucket_start\", how=\"left\")\n",
    "    out[\"is_missing_bucket\"] = out[\"hits\"].isna().astype(\"int8\")\n",
    "\n",
    "    ss, ee = GAP_STORM_START.floor(freq), GAP_STORM_END.floor(freq)\n",
    "    out[\"is_gap_storm\"] = ((out[\"bucket_start\"] >= ss) & (out[\"bucket_start\"] < ee)).astype(\"int8\")\n",
    "\n",
    "    is_m = out[\"is_missing_bucket\"].astype(bool)\n",
    "    run_id = (is_m != is_m.shift()).cumsum()\n",
    "    min_len = int((UNKNOWN_GAP_MIN_HOURS*60) / (pd.Timedelta(freq).total_seconds()/60))\n",
    "    out[\"is_gap_unknown\"] = (is_m & (is_m.groupby(run_id).transform(\"sum\") >= min_len)).astype(\"int8\")\n",
    "\n",
    "    out[\"is_gap\"] = ((out[\"is_gap_storm\"]==1) | (out[\"is_gap_unknown\"]==1)).astype(\"int8\")\n",
    "\n",
    "    for c in FILL_COLS:\n",
    "        out.loc[(out[\"is_gap\"]==0) & (out[c].isna()), c] = 0\n",
    "        out.loc[out[\"is_gap\"]==1, c] = np.nan\n",
    "    return out\n",
    "\n",
    "def rep(split, k, df):\n",
    "    print(\n",
    "        f\"{split}/{k} | rows={len(df):,} | range={df.bucket_start.min()} -> {df.bucket_start.max()} | \"\n",
    "        f\"missing={int(df.is_missing_bucket.sum()):,} | gap={int(df.is_gap.sum()):,} \"\n",
    "        f\"(storm={int(df.is_gap_storm.sum()):,}, unknown={int(df.is_gap_unknown.sum()):,})\"\n",
    "    )\n",
    "\n",
    "for split, raw in [(\"train\", raw_train), (\"test\", raw_test)]:\n",
    "    save_dir = SAVE_TRAIN_DIR if split == \"train\" else SAVE_TEST_DIR\n",
    "\n",
    "    for k, freq in FREQS.items():\n",
    "        out = to_ts3(agg(raw, freq), freq)\n",
    "\n",
    "        # 1) save to outputs/02_eda\n",
    "        p1 = OUT_DIR / f\"ts3_{split}_{k}.parquet\"\n",
    "        out.to_parquet(p1, index=False)\n",
    "\n",
    "        # 2) save to data/train or data/test\n",
    "        p2 = save_dir / f\"ts3_{k}.parquet\"\n",
    "        out.to_parquet(p2, index=False)\n",
    "\n",
    "        rep(split, k, out)\n",
    "        print(\"  saved:\", p1)\n",
    "        print(\"  saved:\", p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78c94839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rows 2934932 test rows 526648\n"
     ]
    }
   ],
   "source": [
    "# (Optional) quick peek\n",
    "print('train rows', len(raw_train), 'test rows', len(raw_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dac605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git add notebooks/*.ipynb\n",
    "# git commit -m \"Update notebooks\"\n",
    "# git push\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
