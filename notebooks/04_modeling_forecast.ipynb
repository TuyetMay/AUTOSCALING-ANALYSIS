{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5083af8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Paths resolved:\n",
      " - REPO_ROOT     : C:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\n",
      " - NOTEBOOKS_DIR : C:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\n",
      " - DATA_DIR      : C:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\data\n",
      " - OUT_DIR       : C:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\n",
      "✅ Cell 1 done — CFG ready (CFG['SCALING'] exists)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 1 — Config & Paths (judge-friendly + reproducible)\n",
    "# - No hard-coded absolute paths\n",
    "# - Works whether you run notebook from repo root or from notebooks/\n",
    "# - DOES NOT copy data anywhere (judge-friendly)\n",
    "# - Uses pathlib for cross-platform paths\n",
    "# =========================\n",
    "\n",
    "import os, re, json, math\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility seed\n",
    "# -------------------------\n",
    "SEED = int(os.environ.get(\"SEED\", \"42\"))\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Paths (robust for Jupyter)\n",
    "# -------------------------\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_repo_root(start: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Find project root by walking up until a folder containing 'notebooks' is found.\n",
    "    Fallback: use current working dir.\n",
    "    \"\"\"\n",
    "    start = start.resolve()\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"notebooks\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "# In Jupyter, __file__ may not exist. Use cwd.\n",
    "CWD = Path.cwd().resolve()\n",
    "REPO_ROOT = _find_repo_root(CWD)\n",
    "\n",
    "# If notebook is in notebooks/, keep NOTEBOOKS_DIR = REPO_ROOT/notebooks\n",
    "NOTEBOOKS_DIR = (REPO_ROOT / \"notebooks\").resolve()\n",
    "\n",
    "# Data/outputs location:\n",
    "# - Prefer notebooks/data if exists\n",
    "# - Else fallback to repo_root/data\n",
    "DATA_DIR = (NOTEBOOKS_DIR / \"data\") if (NOTEBOOKS_DIR / \"data\").exists() else (REPO_ROOT / \"data\")\n",
    "\n",
    "# Outputs always under notebooks/outputs to match your current structure\n",
    "OUT_DIR = NOTEBOOKS_DIR / \"outputs\"\n",
    "OUT_02 = OUT_DIR / \"02_eda\"\n",
    "OUT_03 = OUT_DIR / \"03_features\"\n",
    "OUT_04 = OUT_DIR / \"04_models\"\n",
    "OUT_04P = OUT_04 / \"predictions\"\n",
    "OUT_05 = OUT_DIR / \"05_scaling\"\n",
    "\n",
    "for p in [OUT_02, OUT_03, OUT_04, OUT_04P, OUT_05]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Provide string paths if later cells use os.path.join / strings\n",
    "PROJECT_ROOT = str(NOTEBOOKS_DIR)  # keep compatible with later cells that expect PROJECT_ROOT as string\n",
    "\n",
    "print(\"✅ Paths resolved:\")\n",
    "print(\" - REPO_ROOT     :\", REPO_ROOT)\n",
    "print(\" - NOTEBOOKS_DIR :\", NOTEBOOKS_DIR)\n",
    "print(\" - DATA_DIR      :\", DATA_DIR)\n",
    "print(\" - OUT_DIR       :\", OUT_DIR)\n",
    "\n",
    "# -------------------------\n",
    "# Core helpers (keep as-is for other cells)\n",
    "# -------------------------\n",
    "def tag_minutes(tag: str) -> int:\n",
    "    return {\"1m\": 1, \"5m\": 5, \"15m\": 15}[tag]\n",
    "\n",
    "def steps_per_day(tag: str) -> int:\n",
    "    return int(24 * 60 / tag_minutes(tag))\n",
    "\n",
    "def steps_per_hour(tag: str) -> int:\n",
    "    return int(60 / tag_minutes(tag))\n",
    "\n",
    "def resolve_roll_windows(tag: str, roll_windows: List[str]) -> Dict[str, int]:\n",
    "    sph = steps_per_hour(tag)\n",
    "    spd = steps_per_day(tag)\n",
    "    out = {}\n",
    "    for w in roll_windows:\n",
    "        if w == \"1h\":\n",
    "            out[w] = 1 * sph\n",
    "        elif w == \"6h\":\n",
    "            out[w] = 6 * sph\n",
    "        elif w == \"1d\":\n",
    "            out[w] = 1 * spd\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported roll window: {w}\")\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# CFG (one source of truth)\n",
    "# -------------------------\n",
    "CFG: Dict[str, Any] = {\n",
    "    # ===== Dataset =====\n",
    "    # Prefer access_log.txt in DATA_DIR; judge just needs to put data in ./data or ./notebooks/data\n",
    "    \"RAW_LOG_PATH\": str(DATA_DIR / \"access_log.txt\"),\n",
    "    \"TAGS\": [\"1m\", \"5m\", \"15m\"],\n",
    "    \"TIME_COL_RAW\": \"timestamp\",\n",
    "    \"TIME_COL_BUCKET\": \"bucket_start\",\n",
    "\n",
    "    # Storm gap (problem statement)\n",
    "    \"STORM_START\": pd.Timestamp(\"1995-08-01 14:52:01\"),\n",
    "    \"STORM_END\":   pd.Timestamp(\"1995-08-03 04:36:13\"),\n",
    "\n",
    "    # ===== Feature engineering =====\n",
    "    \"LAG_DAYS\": [1,2,3,4,5,6,7],\n",
    "    \"ROLL_WINDOWS\": [\"1h\",\"6h\",\"1d\"],\n",
    "    \"ROLL_USE_STD\": True,\n",
    "    \"USE_CYCLIC\": True,\n",
    "    \"HORIZON_STEPS\": 1,\n",
    "    \"KEEP_RAW_EXTRA\": [\n",
    "        \"unique_hosts\",\"err_4xx\",\"err_5xx\",\"error_rate\",\n",
    "        \"is_missing_bucket\",\"is_gap_storm\",\"is_gap_unknown\"\n",
    "    ],\n",
    "    \"REQUIRE_COLS\": [\"bucket_start\",\"hits\",\"bytes_sum\",\"is_gap\"],\n",
    "\n",
    "    # ===== Modeling =====\n",
    "    \"TARGETS\": [\"hits\", \"bytes_sum\"],\n",
    "    \"XGB_PARAMS\": dict(\n",
    "        booster=\"gbtree\",\n",
    "        n_estimators=5000,\n",
    "        early_stopping_rounds=50,\n",
    "        objective=\"reg:squarederror\",\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=SEED,\n",
    "    ),\n",
    "    \"CV_SPLITS\": 5,\n",
    "    \"CV_TEST_DAYS\": 2,\n",
    "    \"CV_GAP_STEPS\": 1,\n",
    "\n",
    "    # ==========================================================\n",
    "    # AUTOSCALING / SIMULATION CONFIG (Window-aware + Metric-aware)\n",
    "    # ==========================================================\n",
    "    \"SCALING\": {\n",
    "        \"min_instances\": 2,\n",
    "        \"max_instances\": 50,\n",
    "        \"cost_per_instance_per_hour\": 0.05,\n",
    "        \"window_minutes\": {\"1m\": 1, \"5m\": 5, \"15m\": 15},\n",
    "        \"safety_buffer_by_metric\": {\"hits\": 0.3, \"bytes_sum\": 0.3},\n",
    "        \"capacity_per_instance\": {\n",
    "            (\"hits\",\"1m\"): 20, (\"hits\",\"5m\"): 100, (\"hits\",\"15m\"): 350,\n",
    "            (\"bytes_sum\",\"1m\"): 350_000, (\"bytes_sum\",\"5m\"): 1_200_000, (\"bytes_sum\",\"15m\"): 3_500_000,\n",
    "        },\n",
    "        \"max_step_change_by_window\": {\"1m\": 6, \"5m\": 10, \"15m\": 15},\n",
    "        \"hysteresis_by_window\": {\n",
    "            \"1m\": {\"high\": 2, \"low\": 6, \"in_margin\": 0.18},\n",
    "            \"5m\": {\"high\": 1, \"low\": 4, \"in_margin\": 0.15},\n",
    "            \"15m\":{\"high\": 1, \"low\": 2, \"in_margin\": 0.12},\n",
    "        },\n",
    "        \"predictive_deadband_by_window\": {\"1m\": 0.5, \"5m\": 0.5, \"15m\": 0.5},\n",
    "        \"cooldown_minutes\": {\"base\": 8, \"spike\": 15},\n",
    "        \"provisioning_by_window\": {\n",
    "            \"1m\": {\"warmup_windows\": 1, \"min_uptime_windows\": 6},\n",
    "            \"5m\": {\"warmup_windows\": 1, \"min_uptime_windows\": 4},\n",
    "            \"15m\":{\"warmup_windows\": 0, \"min_uptime_windows\": 2},\n",
    "        },\n",
    "        \"reactive\": {\n",
    "            \"enabled\": True,\n",
    "            \"overload_scale_out_immediate\": True,\n",
    "            \"rescue_extra_instances\": 3,\n",
    "            \"queue_low_fraction\": 0.05,\n",
    "            \"queue_high_multiplier\": 4.0,\n",
    "        },\n",
    "        \"slo\": {\n",
    "            \"base_latency_ms\": 80.0,\n",
    "            \"alpha_latency_per_unit_queue\": 0.15,\n",
    "            \"p95_latency_target_ms\": 300.0,\n",
    "        },\n",
    "        \"anomaly\": {\n",
    "            \"enabled\": True,\n",
    "            \"method\": \"mad\",\n",
    "            \"lookback_hours\": 2,\n",
    "            \"mad_k\": 6.0,\n",
    "            \"min_points\": 10,\n",
    "            \"max_flag_rate\": 0.30,\n",
    "        },\n",
    "        \"ddos_mode\": {\n",
    "            \"enabled\": True,\n",
    "            \"force_scale_out_step_by_window\": {\"1m\": 6, \"5m\": 10, \"15m\": 12},\n",
    "            \"max_instances_during_ddos\": 50,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Cell 1 done — CFG ready (CFG['SCALING'] exists)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2720c6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running XGB...\n",
      "xgb hits 1m {'RMSE': 21.58896693193818, 'MSE': 466.08349318832023, 'MAE': 15.58200884936115, 'MAPE': 64.46296418365739}\n",
      "xgb hits 5m {'RMSE': 70.97562041973819, 'MSE': 5037.538693966758, 'MAE': 52.60916608459284, 'MAPE': 38.35502687318375}\n",
      "xgb hits 15m {'RMSE': 158.22128171880073, 'MSE': 25033.973988740105, 'MAE': 112.97181200400979, 'MAPE': 20.767362835399872}\n",
      "xgb bytes_sum 1m {'RMSE': 699665.340345756, 'MSE': 489531588481.1426, 'MAE': 463251.91777876223, 'MAPE': 88.17260670865876}\n",
      "xgb bytes_sum 5m {'RMSE': 1862245.8911025848, 'MSE': 3467959758928.46, 'MAE': 1315083.320276317, 'MAPE': 43.37298572199524}\n",
      "xgb bytes_sum 15m {'RMSE': 3885168.1435004836, 'MSE': 15094531503270.996, 'MAE': 2825506.901443004, 'MAPE': 31.80724978916396}\n",
      "Running seasonal_naive...\n",
      "seasonal_naive hits 1m {'RMSE': 24.292722225260516, 'MSE': 590.1363531136661, 'MAE': 18.19160429045451, 'MAPE': 73.94465300387039}\n",
      "seasonal_naive hits 5m {'RMSE': 92.80845594235315, 'MSE': 8613.409494403704, 'MAE': 67.92589733693555, 'MAPE': 50.23638202086358}\n",
      "seasonal_naive hits 15m {'RMSE': 246.3389312744002, 'MSE': 60682.86906141367, 'MAE': 175.67323290845886, 'MAPE': 35.71768040771657}\n",
      "seasonal_naive bytes_sum 1m {'RMSE': 682431.5715253976, 'MSE': 465712849814.6239, 'MAE': 480981.6185662474, 'MAPE': 182.4264938862009}\n",
      "seasonal_naive bytes_sum 5m {'RMSE': 2069815.0155896868, 'MSE': 4284134198760.5356, 'MAE': 1548109.961404863, 'MAPE': 69.036400978596}\n",
      "seasonal_naive bytes_sum 15m {'RMSE': 4955225.887284351, 'MSE': 24554263594012.977, 'MAE': 3687770.966396292, 'MAPE': 48.632790298025306}\n",
      "\n",
      "Benchmark (TEST) long->wide\n",
      "   target window metric  seasonal_naive          xgb\n",
      "bytes_sum    15m    MAE    3.687771e+06 2.825507e+06\n",
      "bytes_sum    15m   MAPE    4.863279e+01 3.180725e+01\n",
      "bytes_sum    15m    MSE    2.455426e+13 1.509453e+13\n",
      "bytes_sum    15m   RMSE    4.955226e+06 3.885168e+06\n",
      "bytes_sum     1m    MAE    4.809816e+05 4.632519e+05\n",
      "bytes_sum     1m   MAPE    1.824265e+02 8.817261e+01\n",
      "bytes_sum     1m    MSE    4.657128e+11 4.895316e+11\n",
      "bytes_sum     1m   RMSE    6.824316e+05 6.996653e+05\n",
      "bytes_sum     5m    MAE    1.548110e+06 1.315083e+06\n",
      "bytes_sum     5m   MAPE    6.903640e+01 4.337299e+01\n",
      "bytes_sum     5m    MSE    4.284134e+12 3.467960e+12\n",
      "bytes_sum     5m   RMSE    2.069815e+06 1.862246e+06\n",
      "     hits    15m    MAE    1.756732e+02 1.129718e+02\n",
      "     hits    15m   MAPE    3.571768e+01 2.076736e+01\n",
      "     hits    15m    MSE    6.068287e+04 2.503397e+04\n",
      "     hits    15m   RMSE    2.463389e+02 1.582213e+02\n",
      "     hits     1m    MAE    1.819160e+01 1.558201e+01\n",
      "     hits     1m   MAPE    7.394465e+01 6.446296e+01\n",
      "     hits     1m    MSE    5.901364e+02 4.660835e+02\n",
      "     hits     1m   RMSE    2.429272e+01 2.158897e+01\n",
      "     hits     5m    MAE    6.792590e+01 5.260917e+01\n",
      "     hits     5m   MAPE    5.023638e+01 3.835503e+01\n",
      "     hits     5m    MSE    8.613409e+03 5.037539e+03\n",
      "     hits     5m   RMSE    9.280846e+01 7.097562e+01\n",
      "         model    target window                                                                                                                                                                path  exists\n",
      "seasonal_naive bytes_sum    15m c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\04_models\\predictions\\pred_bytes_sum_15m_seasonal_naive.csv    True\n",
      "           xgb bytes_sum    15m            c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\04_models\\predictions\\pred_bytes_sum_15m_xgb.csv    True\n",
      "seasonal_naive bytes_sum     1m  c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\04_models\\predictions\\pred_bytes_sum_1m_seasonal_naive.csv    True\n",
      "           xgb bytes_sum     1m             c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\04_models\\predictions\\pred_bytes_sum_1m_xgb.csv    True\n",
      "seasonal_naive bytes_sum     5m  c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\04_models\\predictions\\pred_bytes_sum_5m_seasonal_naive.csv    True\n",
      "           xgb bytes_sum     5m             c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\04_models\\predictions\\pred_bytes_sum_5m_xgb.csv    True\n",
      "seasonal_naive      hits    15m      c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\04_models\\predictions\\pred_hits_15m_seasonal_naive.csv    True\n",
      "           xgb      hits    15m                 c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\04_models\\predictions\\pred_hits_15m_xgb.csv    True\n",
      "seasonal_naive      hits     1m       c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\04_models\\predictions\\pred_hits_1m_seasonal_naive.csv    True\n",
      "           xgb      hits     1m                  c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\04_models\\predictions\\pred_hits_1m_xgb.csv    True\n",
      "seasonal_naive      hits     5m       c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\04_models\\predictions\\pred_hits_5m_seasonal_naive.csv    True\n",
      "           xgb      hits     5m                  c:\\Users\\PC\\OneDrive - National Economics University\\Máy tính\\SC\\AUTOSCALING-ANALYSIS\\notebooks\\outputs\\04_models\\predictions\\pred_hits_5m_xgb.csv    True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 04_modeling_forecast.ipynb\n",
    "# Runs XGB + Seasonal Naive, writes outputs/04_models/metrics_forecast.csv and pred files in outputs/04_models/predictions\n",
    "\n",
    "import os, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# =========================\n",
    "# PATHS (judge-friendly) + REPRODUCIBLE\n",
    "# Put this at TOP of 04_modeling_forecast.ipynb\n",
    "# =========================\n",
    "\n",
    "# Reproducible\n",
    "SEED = int(os.environ.get(\"SEED\", \"42\"))\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Detect where we are running from:\n",
    "# - If run inside notebooks/: BASE_DIR = notebooks\n",
    "# - If run from repo root: BASE_DIR = repo_root/notebooks (preferred)\n",
    "cwd = Path.cwd()\n",
    "if cwd.name.lower() == \"notebooks\":\n",
    "    BASE_DIR = cwd\n",
    "elif (cwd / \"notebooks\").exists():\n",
    "    BASE_DIR = cwd / \"notebooks\"\n",
    "else:\n",
    "    # fallback: assume current is notebooks-like\n",
    "    BASE_DIR = cwd\n",
    "\n",
    "# Use OUT_03 generated by CELL 5 (features)\n",
    "OUT_03 = Path(os.environ.get(\"OUT_03\", str(BASE_DIR / \"outputs\" / \"03_features\")))\n",
    "OUT_04 = Path(os.environ.get(\"OUT_04\", str(BASE_DIR / \"outputs\" / \"04_models\")))\n",
    "OUT_04P = OUT_04 / \"predictions\"\n",
    "OUT_04.mkdir(parents=True, exist_ok=True)\n",
    "OUT_04P.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "METRICS_PATH = OUT_04 / \"metrics_forecast.csv\"\n",
    "\n",
    "# (Optional but recommended) avoid duplicated rows when re-run notebook many times\n",
    "# delete old metrics file if exists (comment out if you prefer append)\n",
    "if METRICS_PATH.exists():\n",
    "    METRICS_PATH.unlink()\n",
    "\n",
    "\n",
    "def mape_threshold(y_true, y_pred, min_y=1.0):\n",
    "    y_true=np.asarray(y_true, dtype=float); y_pred=np.asarray(y_pred, dtype=float)\n",
    "    mask=np.abs(y_true)>=float(min_y)\n",
    "    if mask.sum()==0: return np.nan\n",
    "    return float(np.mean(np.abs((y_true[mask]-y_pred[mask])/y_true[mask]))*100.0)\n",
    "\n",
    "def compute_metrics(y_true, y_pred, target):\n",
    "    y_true=np.asarray(y_true, dtype=float); y_pred=np.asarray(y_pred, dtype=float)\n",
    "    mse=float(mean_squared_error(y_true,y_pred))\n",
    "    rmse=float(np.sqrt(mse))\n",
    "    mae=float(mean_absolute_error(y_true,y_pred))\n",
    "    mape=mape_threshold(y_true,y_pred, min_y=1024.0 if target=='bytes_sum' else 1.0)\n",
    "    return {\"RMSE\":rmse,\"MSE\":mse,\"MAE\":mae,\"MAPE\":mape}\n",
    "\n",
    "def write_metrics_long(rows):\n",
    "    dfm=pd.DataFrame(rows)\n",
    "    write_header=not os.path.exists(METRICS_PATH)\n",
    "    dfm.to_csv(METRICS_PATH, mode=\"a\", header=write_header, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "def tag_minutes(tag): return {\"1m\":1,\"5m\":5,\"15m\":15}[tag]\n",
    "def steps_per_day(tag): return int(24*60/tag_minutes(tag))\n",
    "\n",
    "# Use CFG from CELL1 if available else minimal\n",
    "if \"CFG\" not in globals():\n",
    "    CFG={\n",
    "        \"TAGS\":[\"1m\",\"5m\",\"15m\"],\n",
    "        \"TARGETS\":[\"hits\",\"bytes_sum\"],\n",
    "        \"CV_SPLITS\":5,\"CV_TEST_DAYS\":2,\"CV_GAP_STEPS\":1,\n",
    "        \"XGB_PARAMS\": dict(\n",
    "            booster=\"gbtree\",\n",
    "            n_estimators=5000,\n",
    "            early_stopping_rounds=50,\n",
    "            objective=\"reg:squarederror\",\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "        )\n",
    "    }\n",
    "\n",
    "def train_xgb_one(tag, target):\n",
    "    meta=json.load(open(os.path.join(OUT_03,f\"meta_{tag}.json\"),\"r\",encoding=\"utf-8\"))\n",
    "    TIME_COL=meta[\"time_col\"]\n",
    "\n",
    "    train=pd.read_parquet(os.path.join(OUT_03,f\"xgb_train_{tag}.parquet\"))\n",
    "    testf=pd.read_parquet(os.path.join(OUT_03,f\"xgb_test_features_{tag}.parquet\"))\n",
    "\n",
    "    train[TIME_COL]=pd.to_datetime(train[TIME_COL], errors=\"coerce\")\n",
    "    testf[TIME_COL]=pd.to_datetime(testf[TIME_COL], errors=\"coerce\")\n",
    "    if getattr(train[TIME_COL].dt,\"tz\",None) is not None: train[TIME_COL]=train[TIME_COL].dt.tz_convert(None)\n",
    "    if getattr(testf[TIME_COL].dt,\"tz\",None) is not None: testf[TIME_COL]=testf[TIME_COL].dt.tz_convert(None)\n",
    "\n",
    "    train=train.sort_values(TIME_COL).reset_index(drop=True)\n",
    "    testf=testf.sort_values(TIME_COL).reset_index(drop=True)\n",
    "\n",
    "    if target==\"hits\":\n",
    "        FEAT_COLS=list(meta[\"hits_feature_cols\"]); LABEL=meta[\"labels\"][\"hits\"]; TRUE_COL=\"hits\"; use_log=False\n",
    "    else:\n",
    "        FEAT_COLS=list(meta[\"bytes_feature_cols\"]); LABEL=meta[\"labels\"][\"bytes_sum\"]; TRUE_COL=\"bytes_sum\"; use_log=True\n",
    "\n",
    "    FEAT_COLS=[c for c in FEAT_COLS if c not in (TIME_COL, TRUE_COL, LABEL)]\n",
    "\n",
    "    # TimeSeriesSplit sizing\n",
    "    freq_min=tag_minutes(tag)\n",
    "    test_size=int(CFG[\"CV_TEST_DAYS\"]*24*60/freq_min)\n",
    "    gap=int(CFG[\"CV_GAP_STEPS\"])\n",
    "    n=len(train)\n",
    "    max_splits=(n-gap)//test_size - 1\n",
    "    n_splits_eff=int(min(CFG[\"CV_SPLITS\"], max(0,max_splits)))\n",
    "\n",
    "    cv_metrics=[]\n",
    "    if n_splits_eff>=2:\n",
    "        tss=TimeSeriesSplit(n_splits=n_splits_eff, test_size=test_size, gap=gap)\n",
    "        for tr_idx, va_idx in tss.split(train):\n",
    "            tr=train.iloc[tr_idx]; va=train.iloc[va_idx]\n",
    "            X_tr,X_va=tr[FEAT_COLS],va[FEAT_COLS]\n",
    "            y_tr=tr[LABEL].astype(float).values\n",
    "            y_va=va[LABEL].astype(float).values\n",
    "            if use_log:\n",
    "                y_tr_fit=np.log1p(np.maximum(y_tr,0.0))\n",
    "                y_va_fit=np.log1p(np.maximum(y_va,0.0))\n",
    "            else:\n",
    "                y_tr_fit,y_va_fit=y_tr,y_va\n",
    "            reg=xgb.XGBRegressor(**CFG[\"XGB_PARAMS\"])\n",
    "            reg.fit(X_tr,y_tr_fit,eval_set=[(X_va,y_va_fit)],verbose=False)\n",
    "            pred_fit=reg.predict(X_va)\n",
    "            pred=np.expm1(pred_fit) if use_log else pred_fit\n",
    "            pred=np.maximum(pred,0.0)\n",
    "            cv_metrics.append(compute_metrics(y_va,pred,target))\n",
    "\n",
    "    cv_mean={k: float(np.mean([m[k] for m in cv_metrics])) if cv_metrics else np.nan for k in [\"RMSE\",\"MSE\",\"MAE\",\"MAPE\"]}\n",
    "\n",
    "    # Retrain full\n",
    "    X_all=train[FEAT_COLS]\n",
    "    y_all=train[LABEL].astype(float).values\n",
    "    y_fit=np.log1p(np.maximum(y_all,0.0)) if use_log else y_all\n",
    "    final_params={k:v for k,v in CFG[\"XGB_PARAMS\"].items() if k!=\"early_stopping_rounds\"}\n",
    "    model=xgb.XGBRegressor(**final_params)\n",
    "    model.fit(X_all,y_fit,eval_set=[(X_all,y_fit)],verbose=False)\n",
    "\n",
    "    model_path=os.path.join(OUT_04,f\"model_xgb_{target}_{tag}.json\")\n",
    "    model.get_booster().save_model(model_path)\n",
    "    json.dump(FEAT_COLS, open(os.path.join(OUT_04,f\"feat_cols_xgb_{target}_{tag}.json\"),\"w\",encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Predict test aligned t->t+1\n",
    "    df=testf[[TIME_COL, TRUE_COL]+FEAT_COLS].copy().sort_values(TIME_COL).reset_index(drop=True)\n",
    "    df=df.loc[:,~df.columns.duplicated()].copy()\n",
    "    df[\"true_next\"]=pd.to_numeric(df[TRUE_COL],errors=\"coerce\").astype(float).shift(-1)\n",
    "    eval_df=df[df[\"true_next\"].notna()].copy()\n",
    "    if len(eval_df)==0:\n",
    "        out0=df[[TIME_COL, TRUE_COL, \"true_next\"]].head(0).copy()\n",
    "        out0[\"pred\"]=np.nan\n",
    "        test_m={k:np.nan for k in [\"RMSE\",\"MSE\",\"MAE\",\"MAPE\"]}\n",
    "    else:\n",
    "        pred_fit=model.predict(eval_df[FEAT_COLS])\n",
    "        pred=np.expm1(pred_fit) if use_log else pred_fit\n",
    "        pred=np.maximum(pred,0.0)\n",
    "        eval_df[\"pred\"]=pred\n",
    "        test_m=compute_metrics(eval_df[\"true_next\"].values, eval_df[\"pred\"].values, target)\n",
    "        out0=eval_df[[TIME_COL, TRUE_COL, \"true_next\", \"pred\"]]\n",
    "\n",
    "    # export preds\n",
    "    csv_path=os.path.join(OUT_04P, f\"pred_{target}_{tag}_xgb.csv\")\n",
    "    pq_path=os.path.join(OUT_04P, f\"pred_{target}_{tag}_xgb.parquet\")\n",
    "    out0.to_csv(csv_path,index=False,encoding=\"utf-8-sig\")\n",
    "    out0.to_parquet(pq_path,index=False)\n",
    "\n",
    "    rows=[]\n",
    "    for split,metrics in [(\"cv_mean\",cv_mean),(\"test\",test_m)]:\n",
    "        for metric_name,v in metrics.items():\n",
    "            rows.append({\"model\":\"xgb\",\"target\":target,\"window\":tag,\"split\":split,\"metric\":metric_name,\"value\":float(v) if v is not None else np.nan})\n",
    "    write_metrics_long(rows)\n",
    "    return test_m\n",
    "\n",
    "# Model 2: seasonal naive\n",
    "def seasonal_naive_forecast(hist, season_len):\n",
    "    hist=np.asarray(hist,dtype=float)\n",
    "    if len(hist)==0: return 0.0\n",
    "    if len(hist)<season_len: return float(hist[-1])\n",
    "    return float(hist[-season_len])\n",
    "\n",
    "def train_seasonal_naive_one(tag, target):\n",
    "    TIME_COL=\"bucket_start\"\n",
    "    train=pd.read_parquet(os.path.join(OUT_03,f\"xgb_train_{tag}.parquet\"))\n",
    "    test_truth=pd.read_parquet(os.path.join(OUT_03,f\"xgb_test_{tag}.parquet\"))\n",
    "    train[TIME_COL]=pd.to_datetime(train[TIME_COL], errors=\"coerce\")\n",
    "    test_truth[TIME_COL]=pd.to_datetime(test_truth[TIME_COL], errors=\"coerce\")\n",
    "    # choose correct true col in truth file\n",
    "    true_col = \"hits_true\" if target==\"hits\" else \"bytes_sum_true\"\n",
    "    te=test_truth.rename(columns={true_col: target}).copy()\n",
    "    te=te.sort_values(TIME_COL).reset_index(drop=True)\n",
    "\n",
    "    te[\"true_next\"]=pd.to_numeric(te[target],errors=\"coerce\").astype(float).shift(-1)\n",
    "    eval_df=te[te[\"true_next\"].notna()].copy().reset_index(drop=True)\n",
    "    if len(eval_df)==0:\n",
    "        out=te[[TIME_COL,target,\"true_next\"]].head(0).copy(); out[\"pred\"]=np.nan\n",
    "        tm={k:np.nan for k in [\"RMSE\",\"MSE\",\"MAE\",\"MAPE\"]}\n",
    "        return tm\n",
    "\n",
    "    hist=pd.to_numeric(train[target],errors=\"coerce\").astype(float).fillna(0.0).values.tolist()\n",
    "    season_len=steps_per_day(tag)\n",
    "    preds=[]\n",
    "    for i in range(len(eval_df)):\n",
    "        y_t=float(pd.to_numeric(te.iloc[i][target],errors=\"coerce\"))\n",
    "        if not np.isfinite(y_t): y_t=0.0\n",
    "        hist.append(y_t)\n",
    "        preds.append(max(0.0, seasonal_naive_forecast(hist, season_len)))\n",
    "    eval_df[\"pred\"]=np.asarray(preds,dtype=float)\n",
    "\n",
    "    csv_path=os.path.join(OUT_04P,f\"pred_{target}_{tag}_seasonal_naive.csv\")\n",
    "    pq_path=os.path.join(OUT_04P,f\"pred_{target}_{tag}_seasonal_naive.parquet\")\n",
    "    eval_df[[TIME_COL,target,\"true_next\",\"pred\"]].to_csv(csv_path,index=False,encoding=\"utf-8-sig\")\n",
    "    eval_df[[TIME_COL,target,\"true_next\",\"pred\"]].to_parquet(pq_path,index=False)\n",
    "\n",
    "    tm=compute_metrics(eval_df[\"true_next\"].values, eval_df[\"pred\"].values, target)\n",
    "    rows=[{\"model\":\"seasonal_naive\",\"target\":target,\"window\":tag,\"split\":\"test\",\"metric\":k,\"value\":float(v) if v is not None else np.nan} for k,v in tm.items()]\n",
    "    write_metrics_long(rows)\n",
    "    return tm\n",
    "\n",
    "# Run all\n",
    "print(\"Running XGB...\")\n",
    "for target in CFG[\"TARGETS\"]:\n",
    "    for tag in CFG[\"TAGS\"]:\n",
    "        tm=train_xgb_one(tag,target)\n",
    "        print(\"xgb\",target,tag,tm)\n",
    "\n",
    "print(\"Running seasonal_naive...\")\n",
    "for target in CFG[\"TARGETS\"]:\n",
    "    for tag in CFG[\"TAGS\"]:\n",
    "        tm=train_seasonal_naive_one(tag,target)\n",
    "        print(\"seasonal_naive\",target,tag,tm)\n",
    "\n",
    "# Benchmark view\n",
    "mdf=pd.read_csv(METRICS_PATH)\n",
    "test_m=mdf[mdf[\"split\"].astype(str).str.lower().eq(\"test\")].copy()\n",
    "bench=test_m.pivot_table(index=[\"target\",\"window\",\"metric\"], columns=[\"model\"], values=\"value\", aggfunc=\"first\").reset_index()\n",
    "print(\"\\nBenchmark (TEST) long->wide\")\n",
    "print(bench.sort_values([\"target\",\"window\",\"metric\"]).to_string(index=False))\n",
    "\n",
    "# Validate pred files exist\n",
    "pred_files=[]\n",
    "for model in [\"xgb\",\"seasonal_naive\"]:\n",
    "    for target in CFG[\"TARGETS\"]:\n",
    "        for tag in CFG[\"TAGS\"]:\n",
    "            fp=os.path.join(OUT_04P,f\"pred_{target}_{tag}_{model}.csv\")\n",
    "            pred_files.append({\"model\":model,\"target\":target,\"window\":tag,\"path\":fp,\"exists\":os.path.exists(fp)})\n",
    "pred_check=pd.DataFrame(pred_files).sort_values([\"target\",\"window\",\"model\"]).reset_index(drop=True)\n",
    "print(pred_check.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
